{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hey, I am  Nithish Duvvuru. </p> <p>My journey began with a fascination for the intricacies of machine intelligence, evolving into a mastery of frameworks such as TensorFlow and PyTorch. I specialize in developing cutting-edge models for image recognition, natural language processing, and predictive analytics, harnessing the power of deep neural networks to extract meaningful insights from vast datasets.</p> <p>My expertise lies in developing advanced algorithms for optical character recognition (OCR), document classification, and information extraction. My work extends beyond model development, encompassing the orchestration of scalable and efficient infrastructures to deploy and manage sophisticated deep learning systems.</p>"},{"location":"Books/UDL/00.%20Introduction/","title":"00. Introduction","text":"<p>In this comprehensive exploration of \"Understanding deep learning\", we delve into core principles, methodologies and applications of deep learning.  From the foundational concepts of neural networks to the advancements in deep reinforcement learning, each chapter is crafted to provide clarity and comprehension. </p>"},{"location":"Books/UDL/05.%20Loss%20Functions/","title":"05. Loss Functions","text":""},{"location":"Books/UDL/05.%20Loss%20Functions/#introduction","title":"Introduction","text":"<p>In machine learning (ML), a loss function, also known as a cost function or objective function, measures the difference between the predicted values generated by a model and the actual ground truth values. Loss functions play a crucial role in training models by quantifying how well the model is performing. The goal during training is to minimize this loss function, thereby improving the model's ability to make accurate predictions.  </p> <p>A loss function or cost function \\(L(\\phi)\\) returns a single number that describes the mismatch between the model predictions \\(f[x_i, \\phi]\\) and their corresponding ground-truth outputs \\(y_i\\).  During training, we seek parameter values \\(\\phi\\) that minimize the loss and hence map the training inputs to the outputs as closely as possible. </p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#maximum-likelihood","title":"Maximum Likelihood","text":"<p>Consider a model \\(f[x, \\phi]\\) with parameters \\(\\phi\\) that computes an output from input x.  Consider the model as computing a conditional probability distribution \\(Pr(y|x)\\) over possible outputs y given input x. The loss encourages each training output \\(y_i\\) to have a high probability under the distribution \\(Pr(y_i | x_i)\\) computed from the corresponding input \\(x_i\\). </p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#computing-a-distribution-over-outputs","title":"Computing a distribution over outputs","text":"<p>This raises the question of exactly how a model  \\( f[x, \\phi] \\) can be adapted to compute a probability distribution. The solution is simple. First, we choose a parametric distribution \\( Pr(y | \\theta) \\)  defined on the output domain y. Then we use the network to compute one or more of the parameters \u03b8 of this distribution.</p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#maximum-likelihood-criterion","title":"Maximum likelihood criterion","text":"<p>The combined probability term is the likelihood of the parameters, and hence equation 5.1 is known as the maximum likelihood criterion. </p> <p>Here we are implicitly making two assumptions. First, we assume that the data are identically distributed. Second, we assume that the conditional distributions \\( Pr(y_i | xi_i) \\) of the output given the input are independent, so the total likelihood of the training data decomposes as</p> <p>$$ Pr(y1,y2,...,yI|x1,x2,...,xI) = \\prod\\limits_{i=1}^{I} Pr(y_i | x_i) $$ In other words, we assume the data are independent and identically distributed (i.i.d.).</p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#maximizing-log-likelihood","title":"Maximizing Log likelihood","text":"<p>The maximum likelihood criterion (equation 5.1) is not very practical. Each term Pr(yi|f[xi,\u03c6]) can be small, so the product of many of these terms can be tiny. It may be di\ufb00icult to represent this quantity with finite precision arithmetic. Fortunately, we can equivalently maximize the logarithm of the likelihood. </p> \\[ \\begin{align}     \\hat \\phi &amp;= \\underset{\\phi}{\\operatorname{argmax}} \\left[ \\prod\\limits_{i=1}^{I} Pr(y_i | f[x_i, \\phi]) \\right ] \\\\     &amp;= \\underset{\\phi}{\\operatorname{argmax}} \\left[ log \\left [\\prod\\limits_{i=1}^{I} Pr(y_i | f[x_i, \\phi]) \\right ] \\right ] \\\\     &amp;= \\underset{\\phi}{\\operatorname{argmax}} \\left [\\sum\\limits_{i=1}^{I} log \\left [ Pr(y_i | f[x_i, \\phi]) \\right ] \\right ] \\end{align} \\] <p>Since logartihmic is monotonically increasing function, It follows that when we change the model parameters \u03c6 to improve the log-likelihood criterion, we also improve the original maximum likelihood criterion. However, the log-likelihood criterion has the practical advantage of using a sum of terms, not a product, so representing it with finite precision isn\u2019t problematic.</p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#minimizing-negative-log-likelihood","title":"Minimizing Negative log likelihood","text":"<p>Finally, we note that, by convention, model fitting problems are framed in terms of minimizing a loss. To convert the maximum log-likelihood criterion to a minimization problem, we multiply by minus one, which gives us the negative log-likelihood criterion</p> \\[ \\begin{align}     \\hat \\phi &amp;= \\underset{\\phi}{\\operatorname{argmin}} \\left [ - \\sum\\limits_{i=1}^{I} log \\left [ Pr(y_i | f[x_i, \\phi]) \\right ] \\right ] \\\\         &amp;= \\underset{\\phi}{\\operatorname{argmin}} [L[\\phi] ] \\end{align} \\]"},{"location":"Books/UDL/05.%20Loss%20Functions/#inference","title":"Inference","text":"<p>The network no longer directly predicts the outputs y but instead determines a proba- bility distribution over y. When we perform inference, we often want a point estimate rather than a distribution, so we return the maximum of the distribution</p> \\[     \\hat y = \\underset{y}{\\operatorname{argmax}} \\left [ Pr(y | f[x, \\phi])\\right ] \\]"},{"location":"Books/UDL/05.%20Loss%20Functions/#univariate-regression","title":"Univariate Regression","text":""},{"location":"Books/UDL/05.%20Loss%20Functions/#least-squares","title":"Least Squares","text":"<p>The goal is to predict a single scalar output \\( y \\in R\\) from input x using a model \\( f(x, \\phi ) \\) with parameters \\( \\phi \\) .  Lets select the probability distribution over the output domain \\( y \\) to be an univariate normal distritbution. This distribution has two parameters \\(\\mu\\) , \\(\\sigma\\) and has probability density function as follows  $$ Pr(y | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}} $$ We set our machine learning model to predict one or more parameters of the distribution. Here we just compute the mean \\(\\mu = f(x, \\phi)\\). So above equation becomes  $$ Pr(y | f(x, \\phi), \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(y - f(x, \\phi))^2}{2\\sigma^2}} $$ Now we aim to find the paramters \\(\\phi\\) to make the training data \\({x_i, y_i }\\) most probable under this distribution. We choose a loss function \\(L(\\phi)\\) based on the negative log likelihood</p> \\[ %underset{&lt;constraints&gt;}{\\operatorname{&lt;argmax or argmin&gt;}} %\\underset{c\\in C}{\\operatorname{argmin}} \\begin {align}     L(\\phi) &amp;= \\sum_{i=1}^{I} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - f(x, \\phi))^2}{2\\sigma^2}\\right)  \\\\         &amp;= \\underset{\\phi}{\\operatorname{argmin}}(\\sum_{i=1}^{I} log [\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - f(x, \\phi))^2}{2\\sigma^2}\\right)) ] \\\\         &amp;= \\underset{\\phi}{\\operatorname{argmin}}(\\sum_{i=1}^{I} log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) -  \\left(\\frac{(y - f(x, \\phi))^2}{2\\sigma^2}\\right)) \\\\         &amp;= \\underset{\\phi}{\\operatorname{argmin}}(\\sum_{i=1}^{I} - \\frac{(y - f(x, \\phi))^2}{2\\sigma^2})) \\\\          &amp;= \\underset{\\phi}{\\operatorname{argmin}}(\\sum_{i=1}^{I} (y - f(x, \\phi))^2) \\end{align} \\] <ul> <li>We have removed the first term because it doesnt depend on model parameters \\(\\phi\\)</li> <li>We have removed the denominator as this is just a scaling factor and it doesnt affect the optimization. </li> <li>We observe that least square loss function follows naturally from the assumptions that prediction errors are independent and drawn from a normal distribution with \\(\\mu = f(x, \\phi)\\) </li> </ul>"},{"location":"Books/UDL/05.%20Loss%20Functions/#inference_1","title":"Inference","text":"<p>The network no longer directly predicts y but instead predicts the mean \\(\u03bc = f[x, \\phi]\\) of the normal distribution over y. When we perform inference, we usually want a single \u201cbest\u201d point estimate \\(\\hat y\\), so we take the maximum of the predicted distribution:</p> <p>$$      \\hat y  =  \\underset{y}{\\operatorname{argmax}} \\left[ Pr(y | f(x, \\hat \\phi, \\sigma^2))\\right] $$ For the univariate normal, the maximum position is determined by the mean parameter \u03bc. This is precisely what the model computed, so \\(\\hat y =f[x, \\hat \\phi ]\\).</p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#estimate-variance","title":"Estimate Variance","text":"<p>To formulate the least squares loss function, we assumed that the network predicted the mean of a normal distribution. In inference, the model predicts the mean \\(\\mu = f[x, \\hat \\phi ]\\)  from the input, and we learned the variance \\(\\sigma^2\\) during the training process. The former is the best prediction. The latter tells us about the uncertainty of the prediction. Minimizing equation becomes </p> \\[ \\hat \\phi, \\hat \\sigma^2 = \\underset{\\phi, \\sigma^2}{\\operatorname{argmin}}(\\sum_{i=1}^{I} log [\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - f(x, \\phi))^2}{2\\sigma^2}\\right)) ] \\]"},{"location":"Books/UDL/05.%20Loss%20Functions/#heteroscedastic-regression","title":"Heteroscedastic regression","text":"<p>The model above assumes that the variance of the data is constant everywhere. However, this might be unrealistic. When the uncertainty of the model varies as a function of the input data, we refer to this as heteroscedastic (as opposed to homoscedastic, where the uncertainty is constant).</p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#binary-classification","title":"Binary Classification","text":"<p>In binary classfication the goal is to assign the data x to one of the discrete classes \\(y \\in {0,1}\\). First we choose a probability distribution over the output space \\(y \\in {0,1}\\). A suitable choice for this is bernouli distribution which has a single parameter \\(\\lambda\\) . \\(\\lambda\\) represents the probability of \\(y=1\\). </p> \\[ Pr(y | \\lambda) = \\begin{cases} \\lambda &amp; \\text{if } y = 1 \\\\ 1 - \\lambda &amp; \\text{if } y = 0 \\end{cases} \\] <p>Above equation can also be represented as  $$      Pr(y | \\lambda)  = (1-\\lambda)^{1-y}. \\lambda^y $$</p> <p>We set the machine learning model to predict single paramter \\(\\lambda\\). \\(\\lambda\\) can take values in the range [0, 1].  We pass the network outputs throught the sigmoid function which maps outputs to be in rangeof [0, 1].  Likelihood is defined as  $$     L(\\phi) = (1-sig[f[x, \\phi]]))^{1-y} . sig[f[x, \\phi]]^y $$</p> <p>Negative log likelihood for a training set would be </p> \\[     L[\\phi] = \\sum\\limits_{i=1}^{I} - (1-y_i) log \\left [(1-sig[f[x, \\phi]])) \\right] - y_i log \\left[sig[f[x, \\phi]]\\right] \\] <p>The transformed model output \\(sig[f[x, \\phi]]\\) predicts the parameter \\(\\lambda\\) of the Bernoulli distribution. This represents the probability that y = 1, and it follows that 1 \u2212 \\(\\lambda\\) represents the probability that y = 0. When we perform inference, we may want a point estimate of y, so we set y = 1 if \\(\\lambda\\) &gt; 0.5 and y = 0 otherwise.</p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#cross-entropy-loss","title":"Cross-entropy loss","text":"<p>The cross-entropy loss is based on the idea of finding parameters \\(\\theta\\) that minimize the distance between the empirical distribution \\(q(y)\\) of the observed data y and a model distribution \\(Pr(y|\\theta)\\)</p> \\[ D_{\\text{KL}}[q || p] = \\int_{-\\infty}^{\\infty} q(z) \\log \\left( \\frac{q(z)}{p(z)} \\right) \\, dz \\] <p>Now consider that we observe an empirical data distribution at points \\({y_i}_{i=1}^{I}\\). We can describe this as a weighted sum of point masses:</p> <p>$$ q(y) = \\frac{1}{I} \\sum\\limits_{i=1}^{I} \\delta[y-y_i] $$ where \\(\\delta\\) is the delta function. We want to minimize the KL divergence between the model distribution \\(Pr(y | \\theta)\\) and this emperical distribution </p> \\[ \\begin {align}     \\hat \\theta &amp;= \\int_{-\\infty}^{\\infty} q(z) \\log \\left( \\frac{q(z)}{p(z)} \\right) \\, dz \\\\         &amp;= \\int_{-\\infty}^{\\infty} q(y) \\log \\left( q(y) \\right) \\, dy - \\int_{-\\infty}^{\\infty} q(y) \\log \\left( Pr(y | \\theta)  \\right) dy  \\\\         &amp;= - \\int_{-\\infty}^{\\infty} q(y) \\log \\left( Pr(y | \\theta)  \\right) dy  \\\\ \\end{align} \\] <p>First term disappears since it doesnt depent on model paramters \\(\\theta\\). Second term is knows as cross entropy. It can be interpreted as the amount of uncertainty that remains in one distribution after taking into account what we already know from the other. </p>"},{"location":"Books/UDL/05.%20Loss%20Functions/#references","title":"References","text":"<ol> <li>Understanding Deep Learning</li> <li>Maximum Likelihood</li> </ol>"},{"location":"Books/UDL/08.%20Measuring%20Performance/","title":"08. Measuring Performance","text":""},{"location":"Books/UDL/08.%20Measuring%20Performance/#introduction","title":"Introduction","text":"<p>With su\ufb00icient capacity (i.e., number of hidden units), a neural network model will often perform perfectly on the training data. However, this does not necessarily mean it will generalize well to new test data. We will see that the test errors have three distinct causes and that their relative contributions depend on (i) the inherent uncertainty in the task, (ii) the amount of training data, and (iii) the choice of model. We discuss how to select both the model hyperparameters  and the learning algorithm hyperparameters. </p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#sources-of-error","title":"Sources of Error","text":"<p>Consider a quasi-sinusoidal function; both training and test data are generated by sampling input values in the range [0, 1], passing them through this function, and adding Gaussian noise with a fixed variance.</p> <ul> <li>Noise<ul> <li>The data generation process includes the addition of noise, so there are multiple possible valid outputs y for each input x.</li> <li>Noise may arise because there is a genuine stochastic element to the data generation process, because some of the data are mislabeled, or because there are further explanatory variables that were not observed.</li> <li>However, noise is usually a fundamental limitation on the possible test performance.</li> </ul> </li> <li>Bias<ul> <li>A second potential source of error may occur because the model is not flexible enough to fit the true function perfectly.</li> <li>For example, the three-region neural network model cannot exactly describe the quasi-sinusoidal function, even when the parameters are chosen optimally (figure 8.5b). This is known as bias.</li> </ul> </li> <li>Variance <ul> <li>We have limited training examples, and there is no way to distinguish sys tematic changes in the underlying function from noise in the underlying data.</li> <li>When we fit a model, we do not get the closest possible approximation to the true underlying function.</li> <li>This additional source of variability in the fitted function is termed variance.  In practice, there might also be additional variance due to the stochastic learning algorithm, which does not necessarily converge to the same solution each time.</li> </ul> </li> </ul>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#reducing-error","title":"Reducing Error","text":"<p>We saw that test error results from three sources: noise, bias, and variance. The noise component is insurmountable; there is nothing we can do to circumvent this, and it represents a fundamental limit on model performance. However, it is possible to reduce the other two terms.</p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#reducing-variance","title":"Reducing Variance","text":"<p>Recall that the variance results from limited noisy training data. Fitting the model to two different training sets results in slightly different parameters. It follows we can reduce the variance by increasing the quantity of training data. This averages out the inherent noise and ensures that the input space is well sampled. In general, adding training data almost always improves test performance.</p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#reducing-bias","title":"Reducing Bias","text":"<p>The bias term results from the inability of the model to describe the true underlying function. This suggests that we can reduce this error by making the model more flexible. This is usually done by increasing the model capacity. For neural networks, this means adding more hidden units and/or hidden layers.</p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#bias-variance-trade-off","title":"Bias-Variance trade-off","text":"<p>For a fixed-size training dataset, the variance term increases as the model capacity increases. Consequently, increasing the model capacity does not necessarily reduce the test error. This is known as the bias-variance trade-off. </p> <p>As we add capacity to the model, the bias decreases, but the variance increases for a fixed-size training dataset. This suggests that there is an optimal capacity where the bias is not too large and the variance is still relatively small. </p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#double-descent","title":"Double Descent","text":"<p>Test error decreases as we add capacity to the model until it starts to overfit the training data. However, then it does something unexpected; it starts to decrease again. Indeed, if we add enough capacity, the test loss reduces to below the minimal level that we achieved in the first part of the curve. This phenomenon is known as double descent.</p> <p>The first part of the curve is referred to as the classical or under-parameterized regime, and the second part as the modern or over-parameterized regime. The central part where the error increases is termed the critical regime.</p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#explanation","title":"Explanation","text":"<p>The discovery of double descent is recent, unexpected, and somewhat puzzling. It results from an interaction of two phenomena. First, the test performance becomes temporarily worse when the model has just enough capacity to memorize the data. Second, the test performance continues to improve with capacity even after the training performance is perfect.</p> <p>Once the model fits the training data almost perfectly, it implies that adding further capacity to the model cannot help to fit the training data better. The tendency of a model to prioritize one solution over another as it extrapolates between data points is known as its inductive bias. </p> <p>The tendency of the volume of high-dimensional space to overwhelm the number of training points is termed the curse of dimensionality. There are small regions of input space where we observe data with significant gaps between them. The putative explanation for double descent is that as we add capacity to the model, it interpolates between the nearest data points increasingly smoothly.</p> <p>It\u2019s certainly true that as we add more capacity to the model, it will have the capability to create smoother functions. When the number of parameters is very close to the number of training data examples, the model is forced to contort itself to fit the training data exactly, resulting in erratic predictions. As we add more hidden units, the model has the ability to construct smoother functions that are likely to generalize better to new data.</p> <p>However, this does not explain why over-parameterized models should produce smooth functions. The answer to this question is uncertain, but there are two likely possibilities. First, the network initialization may encourage smoothness, and the model never departs from the sub-domain of smooth function during the training process. Second, the training algorithm may somehow \u201cprefer\u201d to converge to smooth functions</p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#choosing-hyper-parameters","title":"Choosing Hyper parameters","text":"<p>In the classical regime, we don\u2019t have access to either the bias (which requires knowledge of the true underlying function) or the variance (which requires multiple independently sampled datasets to estimate).  In the modern regime, there is no way to tell how much capacity should be added before the test error stops improving. This raises the question of exactly how we should choose model capacity in practice. </p> <p>For a deep network, the model capacity depends on the numbers of hidden layers and hidden units per layer as well. Furthermore, the choice of learning algorithm and any associated parameters also affects the test performance.</p> <p>The hyperparameter space is generally smaller than the parameter space but still too large to try every combination exhaustively. Hyperparameter optimization algorithms intelligently sample the space of hyperparameters, contingent on previous results. This procedure is computationally expensive since we must train an entire model and measure the validation performance for each combination of hyperparameters.</p>"},{"location":"Books/UDL/08.%20Measuring%20Performance/#references","title":"References","text":"<ol> <li>Understanding Deep Learning</li> <li>MNIST-1D</li> <li>Bias-Variance Trade off Visualization</li> </ol>"},{"location":"Books/UDL/12.%20Transformers/","title":"12. Transformers","text":""},{"location":"Books/UDL/12.%20Transformers/#introduction","title":"Introduction","text":"<p>In recent years, the Transformer architecture has emerged as a game-changer in the field of natural language processing (NLP), transforming the landscape of machine translation, text generation, and a myriad of other tasks. Originally introduced by Vaswani et al. in their seminal paper \"Attention Is All You Need\" in 2017, the Transformer has rapidly become the de facto standard for state-of-the-art NLP models. </p> <p>Transformer architecture comprises of several key components, including the encoder, decoder, self-attention mechanism, multi-head attention, positional encoding, and feed-forward neural network. Let's delve deeper into the intricacies of the aforementioned components.</p>"},{"location":"Books/UDL/12.%20Transformers/#dot-product-self-attention","title":"Dot Product Self Attention","text":"<p>A standard neural network layer \\(f[x]\\) takes a D \u00d7 1 input x and applies a linear transformation followed by an activation function like a ReLU, so      $$ f[x] = ReLU[\\beta + \\Omega [x]] $$ where \\(\\beta\\) contains biases and \\(\\Omega\\) contains the weights</p> <p>Self Attention block \\(sa[.]\\) takes N inputs \\(x_1, x_2...x_N\\) each of dimension Dx1 and returns N output vectors of same size. In the context of NLP, each input represents a word or word fragment.  Set of values is computed for each input  $$ v_m = \\beta_v + \\Omega_vx_m $$ Then the \\(n^{th}\\) output is weighted sum of all the values \\(v_1, v_2, .... v_N\\). $$ sa_n[x_1, ..., x_N]= \\sum\\limits_{m=1}^{N} a[x_m, x_n]v_m $$  The scalar weight \\(a[x_m, x_n]\\) is the attention that \\(n^{th}\\) output pays to input \\(x_m\\). N weights \\(a[.,x_n]\\) are non-negative and sum to 1.  Hence, self-attention can be thought of as routing the values in different proportions to create each output. </p>"},{"location":"Books/UDL/12.%20Transformers/#computing-and-weighting-values","title":"Computing and Weighting Values","text":"<p>Weights \\(\\Omega_v\\) and Biases \\(\\beta_v\\) are applied to each input.  Hence it scales linearly with sequence length N unlike fully connected layers mapping all D*N inputs at once. The value computation can be viewed as a sparse matrix operation with shared parameters. </p> <p>Attention weights \\(a[x_m, x_n]\\) combine the values from different inputs. It follows that the number of attention weights has a quadratic dependence on the sequence length N, but is independent of the length D of each input \\(x_n\\). </p>"},{"location":"Books/UDL/12.%20Transformers/#computing-attention-weights","title":"Computing Attention Weights","text":"<p>We saw that the outputs result from two chained linear transformations.  Value vectors \\(\\beta_v + \\Omega_vx_m\\) are computed independently for each input \\(x_m\\).  These vectors are linearly combined by the attention weights \\(a[x_m, x_n]\\). </p> <p>To compute the attention weights we apply two more linear transformation to the inputs  $$     \\begin{align}         q_n = \\beta_q + \\Omega_qx_n \\         k_m = \\beta_k + \\Omega_kx_m     \\end{align} $$</p> <p>where \\(q_n\\) and \\(k_m\\) are called queries and keys respectively.  We compute the dot product of queries and keys and pass that through softmax function. </p> \\[     \\begin{align}         a[x_m, x_n] &amp;= softmax[k_.^T . q_n] \\\\                     &amp;= \\frac{exp[k_m^T . q_n]}{\\sum\\limits_{m^{1}=1}^{N} exp[k_{m^1}^T] . q_n}     \\end{align} \\] <p>For each \\(x_n\\) attention scores are positive and sum to 1. This is known as dot product self attention. </p> <p>Dot product returns the similarity of its inputs. So the attention scores \\(a[x_., x_n]\\) depend on the relative similarity between \\(n^{th}\\) query and all of the keys. Softmax operation means the vectors compete to contribute to the final result. Queries and Keys must be of same dimension and this dimension can differ from that of values. </p>"},{"location":"Books/UDL/12.%20Transformers/#extensions-to-dot-product-self-attention","title":"Extensions to dot-product self-attention","text":"<p>In dot product self attention,  the computation is the same regardless of the order of the inputs \\(x_n\\). However, order is important when the inputs correspond to the words in a sentence. </p>"},{"location":"Books/UDL/12.%20Transformers/#positional-encoding","title":"Positional encoding","text":"<p>Positional encoding is a technique used to inject information about the position of tokens in a sequence into the input embeddings. The positional encoding is added to the input embeddings before feeding them into the Transformer encoder or decoder. </p> <p>One commonly used method for positional encoding is the sine and cosine functions. The positional encoding matrix P for a sequence of length L and embedding dimension d is computed as follows:</p> \\[ \\begin{align*} P_{(pos, 2i)} = \\sin\\left(\\frac{{pos}}{{10000^{2i/d}}}\\right) \\\\  P_{(pos, 2i+1)} = \\cos\\left(\\frac{{pos}}{{10000^{2i/d}}}\\right) \\end{align*} \\] <p>Where:</p> <ul> <li>pos is the position of the token in the sequence.</li> <li>i is the dimension of the embedding.</li> <li>d is the embedding dimension.</li> </ul> <p>Using positional encoding, Transformers can effectively capture the sequential order of tokens in the input sequence, enabling them to process sequential data such as natural language text more effectively.</p>"},{"location":"Books/UDL/12.%20Transformers/#scaled-dot-product-self-attention","title":"Scaled dot product self-attention","text":"<p>The dot products in the attention computation can have large magnitudes and move the arguments to the softmax function into a region where the largest value completely dominates. Small changes to the inputs to the softmax function now have little effect on the output making the model di\ufb00icult to train. To prevent this, the dot products are scaled by the square root of the dimension \\(D_q\\) of the queries and keys. </p>"},{"location":"Books/UDL/12.%20Transformers/#multiple-heads","title":"Multiple heads","text":"<p>Multiple self-attention mechanisms are usually applied in parallel, and this is known as multi-head self-attention. Now H different sets of values, keys, and queries are computed:</p> \\[     \\begin{align}         V_h &amp;= \\beta_{vh}1^T + \\Omega_{vh}X \\\\          Q_h &amp;= \\beta_{qh}1^T + \\Omega_{qh}X \\\\          K_h &amp;= \\beta_{kh}1^T + \\Omega_{kh}X \\\\     \\end{align} \\] \\[     sa_h[x] = V_h. Softmax[\\frac{K_h^T . Q_h}{\\sqrt D_q}] \\] <p>where we have different parameters  of values - \\((\\beta_{vh}, \\Omega_{vh})\\), queries -\\((\\beta_{qh}, \\Omega_{qh})\\), Keys - \\((\\beta_{kh}, \\Omega_{kh})\\) for each head. </p> <p>Typically, if the dimension of the inputs \\(x_m\\) is D and there are H heads, the values, queries, and keys will all be of size D/H.  The outputs of these self-attention mechanisms are vertically concatenated, and another linear transform \\(\\Omega_c\\) is applied to combine them. </p> <p>Multiple heads seem to be necessary to make self-attention work well. It has been speculated that they make the self-attention network more robust to bad initializations.</p>"},{"location":"Books/UDL/12.%20Transformers/#transformer-layers","title":"Transformer Layers","text":"<p>Transformer layer consists of multi-head self attention unit followed by a fully  connected network \\(mlp[x_.]\\).  Both units are residual networks. In addition, it is typical to add a LayerNorm operation after both the self-attention and fully connected networks. This is similar to BatchNorm but uses statistics across the tokens within a single input sequence to perform the normalization. </p> <p>Transformer layer can be described as </p> \\[ \\begin{align}     X &amp;\\leftarrow X + MhSa[X] \\\\     X &amp;\\leftarrow LayerNorm[x] \\\\      x_n &amp;\\leftarrow x_n + mlp[x_n] \\\\      X &amp;\\leftarrow LayerNorm[X] \\\\  \\end{align} \\] <p>where the column vectors \\(x_n\\)are separately taken from the full data matrix X. </p>"},{"location":"Books/UDL/12.%20Transformers/#transformers-for-nlp","title":"Transformers for NLP","text":"<p>A typical NLP pipeline starts with a tokenizer that splits the text into words or word fragments. Then each of the token is mapped to a learned embedding. These embeddings are passed through a series of transformer layers</p> <ul> <li>Tokenization<ul> <li>This splits the text into smaller constituent units (tokens) from a vocabulary of possible tokens.  </li> <li>In practice, a compromise between letters and full words is used, and the final vocabulary includes both common words and word fragments from which larger and less frequent words can be composed.</li> <li>The vocabulary is computed using a sub-word tokenizer such as byte pair encoding  that greedily merges commonly occurring sub-strings based on their frequency.</li> </ul> </li> <li>Embeddings<ul> <li>Each token in the vocabulary V is mapped to a unique word embedding, and the embed- dings for the whole vocabulary are stored in a matrix \\(\\Omega_e \\in R^{D \\times |V|}\\)</li> <li>The input embeddings are computed as \\(X=\\Omega_e T\\) and \\(\\Omega_e\\) is learned like any other network parameter. </li> <li>A typical embedding size D is 1024, and a typical total vocabulary size |V| is 30,000, so even before the main network, there are many parameters in \\(\\Omega_e\\) to learn.</li> </ul> </li> <li>Transformer<ul> <li>Finally, the embedding matrix X representing the text is passed through a series of K transformer layers, called a transformer model. </li> <li>There are three types of transformer models.<ul> <li>An encoder transforms the text embeddings into a representation that can support a variety of tasks</li> <li>A decoder predicts the next token to continue the input text </li> <li>Encoder-decoders are used in sequence-to-sequence tasks, where one text string is converted into another. </li> </ul> </li> </ul> </li> </ul>"},{"location":"Books/UDL/12.%20Transformers/#references","title":"References","text":"<ol> <li>Understanding Deep Learning</li> <li>Positional Encoding </li> <li>The Annotated Transformer</li> <li>Transformer</li> </ol>"},{"location":"Books/transformers/0.intro/","title":"0. Introduction","text":"<p>Transformers for Machine Learning - A Deep Dive is a comprehensive reference book that provides readers with an in-depth exploration of the Transformer Architecture.</p> <p>Transformers have gone through many adaptations and alterations, resulting in newer techniques and methods. This is the first comprehensive book on transformers.</p> <p>I have compiled these notes as a personal reference for my own study. I hope they will be of use to others. </p>"},{"location":"Books/transformers/1.transformer/","title":"1. Transformer","text":""},{"location":"Books/transformers/1.transformer/#encoder-decoder-architecture","title":"Encoder Decoder Architecture","text":"<p>Many NLP problems, such as machine translation, question answering, and text summarization use pairs of variable length sequences as inputs to train the model. Encoder-Decoder architecture is used to solve these tasks. Encoder takes in the sequences and converts into fixed length output state. Decoder takes a fixed-length state and converts it back into a variable-length output.</p>"},{"location":"Books/transformers/1.transformer/#encoder","title":"Encoder","text":"<ul> <li>Input sentence is tokenized into words and words are mapped into feature vectors. </li> <li>The state \\(h_t\\) known as the context variable or the context vector encodes the information of the entire input sequence. </li> <li>RNN can be bidirectional and thus the hidden state would not only depend on the previous hidden state \\(h_{t\u22121}\\) and input xt, but also on the next state \\(h_{t+1}\\).</li> </ul>"},{"location":"Books/transformers/1.transformer/#decoder","title":"Decoder","text":"<ul> <li>Decoder has the output of the encoder, the context variable c, and the given output sequence to generate the decoded outputs.  </li> </ul>"},{"location":"Books/transformers/1.transformer/#training","title":"Training","text":"<ul> <li>Decoder predicts a probability distribution for the output tokens at each time step, and the softmax gives the distribution over the words. </li> <li>Encoder and decoder are jointly trained, and the cross-entropy loss is used for optimization.</li> <li>Teacher forcing is a strategy to train RNN that uses ground truth  as input instead of prior decoded output. </li> <li>Teacher forcing helps in addressing the slow convergence and instability problems when training RNNs.</li> </ul>"},{"location":"Books/transformers/1.transformer/#issues","title":"Issues","text":"<ul> <li>Information bottleneck </li> <li>Length of words can vary at inference time. </li> <li>Difficult to parallellize. </li> <li>Vanishing/Exploding gradients. </li> </ul>"},{"location":"Books/transformers/1.transformer/#attention","title":"Attention","text":"<ul> <li>Attention mechanism involves selectively focusing on specific elements while filtering out the less relevant ones. </li> <li>Attention mechanism can be considered as a memory with keys and values and a layer which, when someone queries it, generates an output from value whose keys map the input. </li> <li>Attention layer measures the similarity between the query and the key using a score function \u03b1 which returns scores \\(a_1, . . . , a_n\\) for keys \\(k_1,...,k_n\\) given by         $$ a_i = \\alpha(q, k_i) $$</li> <li> <p>Dot Product </p> <ul> <li>Dot product-based scoring function is the simplest one and has no parameters to tune $$ \\alpha(q, k) = q . k $$</li> </ul> </li> <li> <p>Scaled Dot Product </p> <ul> <li>scaled dot product-based scoring function divides the dot product by \\(\\sqrt d_k\\) to remove the influence of dimension of \\(d_k\\).  $$ \\alpha(q, k) = \\frac{q . k}{\\sqrt d_k} $$</li> </ul> </li> <li> <p>Attention weights are computed as a softmax function on the scores \\(b = softmax(a)\\)</p> </li> <li> <p>Final output is weighted sum of attention weights and the values.          $$ o = \\sum\\limits_{i}^{n} b_i v_i $$</p> </li> </ul>"},{"location":"Books/transformers/1.transformer/#transformer","title":"Transformer","text":"<ul> <li>Transformer combines the advantages of convolutional neural networks (CNN) to parallelize the computations and recurrent neural networks (RNN) to capture long-range, variable-length sequential information.</li> <li>Transformer architecture, to gain speed and parallelism, recurrent neural networks are replaced by multi-head attention layers. </li> <li>Word Embeddings - Lookup for tokens in a sentence to convert a sentence of length l, to a matrix W of dimension \\(lxd\\)</li> <li>Positional Encoding <ul> <li>By taking one word at a time, recurrent neural networks essentially in- corporate word order.</li> <li>PE Requirements  - Unique encoding value for each time-step, Consitent distance between two time steps across sentences,  independent of the length of the sentence, deterministic</li> </ul> </li> </ul>"},{"location":"Books/transformers/1.transformer/#attention","title":"Attention","text":"<ul> <li> <p>Self attention</p> <ul> <li>Inputs i.e \\(x_i\\) are converted to the output vectors \\(z_i\\), through the self-attention layer. </li> <li>Each input vector \\(x_i\\), generates three different vectors: the query, key, and value, \\((q_i, k_i, v_i)\\). </li> <li>query, key, and value vectors are obtained by projecting the input vector \\(x_i\\), at time i on the learnable weight matrices \\(W_q, W_k, W_v\\) to get \\(q_i\\), \\(k_i\\), and \\(v_i\\), respectively. </li> <li>Key Roles<ul> <li>Query vector of token i i.e  \\(q_i\\), is to combine with every other key vectors \\(\\sum\\limits_{j =0}^{l} q_i k_j^T\\) to influence the weights for its own output \\(z_i\\).</li> <li>Key vector of token i i.e \\(k_i\\), is to be matched with every other query vectors to get similarity with query and to influence the output through query-key product scoring. </li> <li>Value vector of token i i.e  \\(v_i\\), is extracting information by combining with the output of the query-key scores to get the output vector \\(z_i\\).</li> </ul> </li> </ul> </li> <li> <p>Multi head Attention</p> <ul> <li>Instead of a single self-attention head, there can be h parallel self-attention heads; this is known as multi-head attention</li> <li>Multi-head attention provides different subspace representations instead of just a single representation for the inputs, which helps capture different aspects of the same inputs.</li> </ul> </li> <li> <p>Masked multi head Attention </p> <ul> <li>We want the decoder to learn from the encoder sequence and a particular decoder sequence, which has been already seen by the model, to predict the next word. </li> <li>For the first layer of the decoder, similar to the sequence-to-sequence architecture, only previous target tokens need to be present and others to be masked. </li> <li>This is implemented by having a masking weight matrix M that has \u2212\u221e for future tokens and 0 for previous tokens.      $$ MA(Q, K, V) = Softmax (\\frac{Q. K^T + M }{\\sqrt d_k}) V $$</li> </ul> </li> </ul>"},{"location":"Books/transformers/1.transformer/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Word order and positions play a crucial role in most of the NLP tasks. By taking one word at a time, recurrent neural networks essentially in- corporate word order.</li> <li>To gain speed and parallelism, recurrent neural networks are replaced by multi-head attention layers in transformers. </li> <li> <p>Requirements</p> <ul> <li>Unique encoding value for each time-step</li> <li>Consistent distance between two time-steps across sentences of various lengths.</li> <li>Encoding results are generalized independent of the length of the sentence</li> <li>The encoding is deterministic.</li> </ul> </li> <li> <p>Word embeddings W and the positional encoding P are added to generate the input representation \\(X=W+P  \\in R^{lxd}\\) .</p> </li> </ul>"},{"location":"Books/transformers/1.transformer/#encoder_1","title":"Encoder","text":"<ul> <li>The encoder block in the transformer consists of N identical layers. </li> <li>Each encoder layer has two layers -  multi-head self-attention mechanism and positionwise fully connected feed-forward network.</li> </ul>"},{"location":"Books/transformers/1.transformer/#decoder_1","title":"Decoder","text":"<ul> <li>The decoder block in the transformer also consists of N identical layers. </li> <li>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. </li> </ul>"},{"location":"Books/transformers/2.bert/","title":"2. BERT","text":"<p>Bidirectional Encoder Representations from Trans- former (BERT)  is considered the onset of a revolution in the field of NLP. BERT uses unlabeled text to pre-train deep bidirectional contextual representations. This resulted in rich pre-trained language models that can be fine-tuned with a simple additional output layer  to produce state-of-the-art performance in NLP tasks. </p>"},{"location":"Books/transformers/2.bert/#architecture","title":"Architecture","text":"<ul> <li> <p>Core Layers</p> <ul> <li>Number of transformer layers</li> <li>Size of hidden representations</li> <li>Number of birectional self attention heads. </li> </ul> </li> <li> <p>Input/Output Representation</p> <ul> <li>BERT's input is designed to represent NLP downstream tasks involving a single or a pair of sentences using the same input representation design. </li> <li>BERT prefixes a special  [CLS] token. The hidden vector of this token in the last BERT layer will be used as an aggregate representation for the entire input sequence.</li> <li>For NLP tasks with paired sentences, BERT concatenates the sentences into one sequence with a separator token [SEP].  This serves as one way BERT uses to distinguish the two sentences.</li> <li>Embeddings<ul> <li>WordPiece Tokenization</li> <li>Token Embedding + Segment Embedding + Positional Embedding </li> </ul> </li> </ul> </li> </ul>"},{"location":"Books/transformers/2.bert/#pretraining","title":"Pretraining","text":"<ul> <li> <p>BERT pre-training involves combined training with both MLM and NSP tasks by optimizing the model parameters over their combined loss function. </p> </li> <li> <p>Masked Language Modeling </p> <ul> <li>Idea is to randomly maskout a percentage of the input sequence tokens, replacing them with the special [MASK] token. During pre-training, the modified input sequence is run through BERT and the output representations of the masked tokens are then fed into a softmax layer. </li> <li>Bidirectional attention of the transformer encoder forces the [MASK] prediction task to use the context provided by the other non-masked tokens in the sequence.</li> <li>BERT is pre-trained with a 15% mask-out rate. Every token in the 15% masked-out tokens is subjected to the following heuristic:<ul> <li>With a probability of 80%, the token is replaced with the special [MASK] token</li> <li>With a probability of 10%, the token is replaced with a random token.</li> <li>With a probability of 10%, the token is left unchanged.</li> </ul> </li> <li>The MLM task uses cross-entropy loss only over the masked tokens and ignores the prediction of all non-masked ones.</li> </ul> </li> <li> <p>Next Sentence Prediction NSP</p> <ul> <li>BERT is fed pairs of sentences and pre-trained to predict if the second sentence should follow the first one in a continuous context.</li> <li>The first sentence is prefixed with the [CLS] token, then the two sentences are delimited by the special token [SEP].</li> <li>Model is given sentence pairs where 50% of the time the second sentence comes after the first sentence and the other 50% the second sentence is a random sentence from the full training corpus.</li> <li>BERT representation of [CLS] token encodes both input sentences. Therefore, NSP pre-training is performed by adding a single layer MLP with softmax atop the [CLS] token representation to predict the binary NSP label</li> </ul> </li> </ul>"},{"location":"Books/transformers/2.bert/#bert-variants","title":"Bert-Variants","text":"<ul> <li>RoBERTa<ul> <li>Robustly Optimized BERT Pre-training Approach For the MLM task, BERT randomly masks token during the data pre-processing stage. Therefore, the masks stay static throughout the entire model training process. </li> <li>RoBERTa on the other hand follows a dynamic masking strategy where masked tokens are randomly chosen for each training epoch.</li> <li>RoBERTa also drops the NSP pre-training task and only uses the dynamic MLM task. Training on longer sequences - Full sentences of at most 512 tokens, are sampled contiguously from one or more documents. </li> <li>Large batch Size - RoBERTa showed that using large mini-batches with increased learning rates during pre-training improves the perplexity of the dynamic MLM task as well as the downstream task performance. </li> </ul> </li> </ul>"},{"location":"Books/transformers/2.bert/#bertopic","title":"BERTopic","text":"<ul> <li>Topic modeling is one of the challenging topics in NLP. The advances in BERT and its variants motivate the NLP community to leverage BERT in topic modeling. </li> <li>Starts by creating the embeddings of the documents of interest using BERT models. </li> <li>Preprocessing divides the document to smaller paragraphs or sentences that are smaller than the token size for the transformer model. </li> <li>Clustering is performed on the document embeddings to cluster all the documents with similar topics together. </li> </ul>"},{"location":"Books/transformers/3.modifications/","title":"3. Modifications","text":""},{"location":"Books/transformers/3.modifications/#lightweight-transformers","title":"LightWeight Transformers","text":"<ul> <li>Funnel-Transformer compresses the output of a transformer encoder layer via pooling before it is passed to the next layer. </li> <li>Encoder - The standard transformer uses the same sequence length for all layers. Funnel-Transformer changes this by placing a pooling layer between the transformer layers to reduce the sequence length. Funnel-Transformer uses mean pooling with stride and window size both set to two.</li> <li>Decoder - To support token-level prediction tasks like  machine translation, Funnel-Transformer has an optional decoder that upsamples the compressed encoder output to a full sequence length</li> <li>When Funnel-Transformer decreases the sequence length and adds more layers, it per- forms better than the standard transformer on text classification. When the sequence length is decreases but the depth is not increased, performance decreases on GLUE text classification datasets.</li> </ul>"},{"location":"Books/transformers/3.modifications/#realformer","title":"RealFormer","text":"<ul> <li>Residual Attention Layer Transformer adds residual scores to the raw attention logits of all attention heads from the previous layer of a transformer.      $$Res. Attn (Q, K, V, Prev) = Softmax (\\frac{QK^T}{\\sqrt d} + Prev) V $$     where Prev is the attention logits from the previous transformer layer. </li> <li>This method can be applied to other transformer architectures, including to decoder layers.</li> <li>RealFormer generally performs better than the stan- dard transformer architecture, including its use in BERT, all without increasing the number of model parameters.</li> </ul>"},{"location":"Books/transformers/3.modifications/#transformer-xl","title":"Transformer XL","text":"<ul> <li>Transformer-XL was introduced because the standard transformer architecture\u2019s fixed-width context window prevents it from learning the model dependencies that lie outside of its fixed window.</li> <li>Transformer-XL can handle dependencies 450% longer than the standard transformer and inference is \u223c1800 times faster than the Transformer.</li> <li>Segment level recurrence- </li> <li>This works by using the previous segment of text as additional context when processing the current segment of text. </li> <li>In the standard transformer, the nth transformer layer takes the out- put of the previous layer (n \u2212 1) as input     $$ h_t^n = Transformer(h_t^{n\u22121})$$</li> <li>When computing the output of the \\(n^{th}\\) transformer layer for the current segment \\(X_{t+1}\\), we have a contribution from \\(h_t^{n\u22121}\\),      $$ h_{t+1}^n = TransformerXL(h_{t+1}^{n\u22121}, h_t^{n\u22121})$$</li> <li>This modified attention incorporates information from the previous input sequence to compute a representation of the current input sequence. </li> <li>The transformer output for the current sequence depends on the transformer output for the previous sequence. The transformer output for the previous sequence depends on the sequence before that one and on the previous transformer layer.</li> </ul>"},{"location":"Books/transformers/3.modifications/#longformer","title":"Longformer","text":"<ul> <li>When calculating self-attention there are usually no restrictions on which positions in the sequence can attend to each other.</li> <li>Longformer changes this by restricting which positions can attend to each other according to specific patterns. </li> <li>This results in sparse attention weights across all heads and corresponds to deleting edges from the attention graph. </li> <li>Sliding Window Attention- </li> <li>Each token in a sequence is given a fixed-sized context window, w, so it can only attend to its neighbors. </li> <li> <p>This simple change reduces the complexity from one that is quadratic in the sequence length to one that is linear in the sequence length, O(Lw). </p> </li> <li> <p>Dilated Sliding Window Attention - </p> </li> <li> <p>This attention pattern extends the width of the sliding window attention by add gaps of size d in the context window. (Similar to stride in conv layers)</p> </li> <li> <p>Global Attention - </p> </li> <li> <p>The global attention pattern chooses lets some tokens attend to any other token in the sequence. In such cases, all tokens in the sequence attend to that token.</p> </li> <li> <p>Longformer decides which tokens are allowed to have global attention based on the training task. Longformer combines this global attention with the sliding window attention.</p> </li> <li>Longformer uses small window sizes for lower layers and larger window sizes for higher layers. This gives the higher layers a hierarchical nature. </li> </ul>"},{"location":"Books/transformers/3.modifications/#reformer","title":"Reformer","text":"<ul> <li>Reformer modifies attention mechanism to reduce memory usage by using reversible residual networks. </li> <li>Reformer can include context windows that are several orders of magnitude larger than a Transformer (up to 1M words).</li> <li>scaled dot-product attention in transformer has time complexity of O(\\(L^2\\)), which becomes prohibitive as the number of tokens in the sequence i.e L, increases.</li> <li>Reformer uses locality-sensitive hashing to reduce the time complexity from O(\\(L^2\\)) to O(L log L).</li> <li>Reformer addressesthe memory usage of the standard transformer using reversible residual layers.</li> </ul>"},{"location":"Books/transformers/4.applications/","title":"4. Applications","text":"<p>Since the initial application of transformers to machine translation, the transformer architecture has been applied to computer vision, audio processing, and video processing, as well as other problems in NLP. </p>"},{"location":"Books/transformers/4.applications/#text-processing","title":"Text Processing","text":""},{"location":"Books/transformers/4.applications/#biobert","title":"BioBERT","text":"<ul> <li>Domain-specific language model constructed by fine- tuning BERT on a large collection of biomedical text.</li> <li>BioBERT outperformed previous models on three NLP tasks useful for biomedical text mining: named entity recognition (NER), relationship extraction, and question answering (QA).</li> <li>Training<ul> <li>The starting point is a pre-trained BERT model. </li> <li>Next step is to pre-train the model on PubMed abstracts and PubMed Central full-text articles. </li> <li>Finally, after the pre-training is over, BioBERT is further fine-tuned on the NER, relationship extraction, and question answering tasks, using task-specific datasets.</li> </ul> </li> </ul>"},{"location":"Books/transformers/4.applications/#scibert","title":"SciBERT","text":"<ul> <li>It uses the BERT architecture, but was trained on the full text and abstracts of 1.4 million papers from the Semantic Scholar website. </li> <li>SciBERT uses WordPiece tokenization with 30,000 tokens, but it does not use BERT\u2019s vocabulary</li> <li>Instead, SciBERT\u2019s vocabulary is built from Semantic Scholar corpus. SciBERT was evaluated on five NLP tasks: NER, PICO extraction, text classification, relationship extraction, and dependency parsing.</li> </ul>"},{"location":"Books/transformers/4.applications/#text-generation","title":"Text generation","text":""},{"location":"Books/transformers/4.applications/#gpt","title":"GPT","text":"<ul> <li>In Generative pre-training,  model is first trained on unsupervised data, in a task-agnostic fashion, and later fine-tuned for a specific task</li> <li>GPT is an autoregressive model, which means it uses inputs from previous steps of a sequence to predict values later in the sequence.</li> <li>GPT was evaluated on four kinds of natural language understanding tasks: natural language inference, question answering, semantic similarity, and text classification.</li> <li> <p>Training</p> <ul> <li>Unsupervised Pretraining</li> <li>Supervised Finetuning</li> </ul> </li> <li> <p>Model was a stack of 12 transformer decoder layers, with 12 masked self-attention heads. Model dimension is 768, the feedforward layers use \\(d_{ff}\\) 3072. Positional embeddings were learned, instead of the fixed embeddings used in the standard Transformer. </p> </li> </ul>"},{"location":"Books/transformers/4.applications/#gpt-2","title":"GPT 2","text":"<ul> <li>First GPT model demonstrated the power of generative pre-training, then GPT-2 showed that a language model can learn specific NLP tasks without being explicitly trained on those tasks. </li> <li>In the standard transformer, the layer norm module comes after the multi-head attention and after the position-wise feedforward network, as part of the residual connection. </li> <li>Layer norm module instead comes before the multi-head attention and before the position-wise feedforward network. </li> <li>Each was evaluated on several language modeling datasets without any additional training. GPT-2 achieved state-of-the-art on seven out of eight datasets. </li> </ul>"},{"location":"Books/transformers/4.applications/#gpt-3","title":"GPT 3","text":"<ul> <li>GPT-3 is part of the trend in transformer language models where an increase in the number of parameters leads to an increase in the language model\u2019s ability to perform downstream tasks with little to no task-specific training.</li> <li>Attention mechanisms in the transformer layers alternated between dense and locally banded sparse patterns.</li> </ul>"},{"location":"Books/transformers/4.applications/#computer-vision","title":"Computer Vision","text":""},{"location":"Books/transformers/4.applications/#vit","title":"ViT","text":"<ul> <li>Given an Image with resolution \\(H \\times W\\), ViT works by breaking the two dimensional image into sequence of N patches with an resolution of \\(P \\times P\\).</li> <li>These sequence of patches is like the sequence of tokens in the standard transformer. </li> <li>Before sending the patch sequence through the embedding layer, a learnable embedding analogous to the [CLS] token in BERT, \\(x_{cls}\\) is prepended onto each patch vector  </li> <li>ViT demonstrates that the inductive biases introduced by CNNs are useful for small datasets, but not for larger ones.</li> <li>Experiments demonstrated that hard-coding the two-dimensional structure of the image patches into the positional encodings does not improve quality. </li> </ul>"},{"location":"Books/transformers/4.applications/#multimodal-learning","title":"MultiModal Learning","text":""},{"location":"Books/transformers/4.applications/#vilbert","title":"VilBERT","text":"<ul> <li>Vision-and-Language BERT (VilBERT) is a joint model for learning task-agnostic representations for image and text. </li> <li>Image is first converted into a stream of a sequence of regions mapped to feature vectors using an object detection network.</li> <li>text follows the normal flow of tokens mapped to positional and word embeddings as a stream of sequences.</li> <li>VilBERT uses the standard trans- former block (TRM) and a co-attention transformer block (Co-TRM) that provides sparse interactions between the two modalities.</li> <li>The Co-TRM module computes query, key, and value matrices similar to the standard transformer block for visual and linguistic representations at any intermediate layer. </li> <li>keys and values from each modality are provided as an input to the other modality\u2019s multi-headed attention block.</li> <li>Multi-headed attention block of Co-TRM produces attention-pooled features for each modality conditioned on the other and enabling joint learning.</li> <li>Output is task- dependent and can be as simple as a multi-layer perceptron (MLP) followed by a soft-max layer to compute a score giving similarity between the image and the text.</li> <li> <p>Training</p> <ul> <li>First, the text and image are trained independently.</li> <li>BERT model is trained end-to-end on a large corpus for two tasks: masked language modeling (MLM) and next sentence pre- diction (NSP).</li> <li>Faster R-CNN-based pre-trained object detection network extracts bounding boxes and their visual features from images and processes them through the network.</li> <li>Conceptual Captions dataset  images and their captions is used for joint learning tasks such as mapping regions in the images to the text and masked word prediction from images. </li> </ul> </li> <li> <p>Model is finetuned for specific taks  such as Visual Question-Answering (VQA), Visual Commonsense Reasoning (VCR), Image Retrieval. </p> </li> </ul>"},{"location":"Computer%20Vision/Concepts/Attention/","title":"Attention","text":"<p>Attention mechanisms in neural networks allow models to focus on relevant parts of the input data when making predictions. They have been widely used in various tasks, including natural language processing, computer vision, and sequence-to-sequence tasks. Here's an explanation of attention in neural networks along with some code examples:</p>"},{"location":"Computer%20Vision/Concepts/Attention/#dot-product-attention","title":"Dot Product Attention","text":"<p>One of the simplest forms of attention is dot product attention. In this mechanism, the attention score between a query vector q and a key vector k is computed as the dot product of the two vectors. The attention score is then normalized using a softmax function to obtain attention weights. Finally, the weighted sum of the value vectors vv is computed using the attention weights.</p> <pre><code>import torch\nimport torch.nn.functional as F\n\ndef dot_product_attention(query, key, value):\n    # Compute attention scores\n    attention_scores = torch.matmul(query, key.transpose(-2, -1))\n\n    # Apply softmax to obtain attention weights\n    attention_weights = F.softmax(attention_scores, dim=-1)\n\n    # Compute weighted sum of value vectors\n    attention_output = torch.matmul(attention_weights, value)\n\n    return attention_output, attention_weights\n</code></pre>"},{"location":"Computer%20Vision/Concepts/Attention/#self-attention","title":"Self-Attention","text":"<p>Self-attention is a mechanism used in the Transformer architecture that allows the model to weigh the importance of different words in a sequence when encoding or decoding. It enables the model to focus on relevant parts of the input sequence and has been crucial in achieving state-of-the-art performance in various natural language processing tasks.</p> <ul> <li> <p>Query, Key, and Value: In self-attention, each word in the input sequence is represented by three vectors: a query vector, a key vector, and a value vector. These vectors are linear transformations of the input word embeddings.</p> </li> <li> <p>Attention Scores: For each word in the input sequence, self-attention computes attention scores that measure the relevance of that word to every other word in the sequence. These scores are computed based on the similarity between the query vector of the current word and the key vectors of all other words.</p> </li> <li> <p>Attention Weights: The attention scores are normalized using a softmax function to obtain attention weights. These weights determine how much each word in the sequence contributes to the representation of the current word.</p> </li> <li> <p>Weighted Sum: The final representation of each word is computed as a weighted sum of the value vectors of all words in the sequence, where the weights are the attention weights.</p> </li> </ul> <pre><code>import torch\nimport torch.nn as nn\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Linear(embed_dim, embed_dim)\n        self.key = nn.Linear(embed_dim, embed_dim)\n        self.value = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        # Compute query, key, and value vectors\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n\n        # Compute attention scores\n        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n\n        # Apply softmax to obtain attention weights\n        attention_weights = F.softmax(attention_scores, dim=-1)\n\n        # Compute weighted sum of value vectors\n        attention_output = torch.matmul(attention_weights, value)\n\n        return attention_output, attention_weights\n</code></pre>"},{"location":"Computer%20Vision/Concepts/Attention/#multi-head-attention","title":"Multi head Attention","text":"<p>Multi-head self-attention is an extension of the self-attention mechanism used in neural networks, particularly in models like the Transformer. It enables the model to capture different aspects of the input sequence simultaneously by computing multiple attention heads in parallel. Each attention head learns different representations of the input sequence, allowing the model to attend to different parts of the sequence and capture different patterns or relationships.</p> <ul> <li> <p>Query, Key, and Value: Like in regular self-attention, each word in the input sequence is represented by three vectors: a query vector, a key vector, and a value vector. These vectors are linear transformations of the input embeddings.</p> </li> <li> <p>Multiple Heads: In multi-head self-attention, the model computes multiple sets of query, key, and value vectors, known as attention heads. Each attention head is capable of learning different relationships between words in the sequence.</p> </li> <li> <p>Parallel Computation: The attention heads are computed in parallel, allowing the model to capture different aspects of the input sequence simultaneously. After computing the attention scores for each attention head, the outputs are concatenated and linearly transformed to obtain the final output.</p> </li> </ul> <pre><code>class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert embed_dim % num_heads == 0\n\n        self.linear_q = nn.Linear(embed_dim, embed_dim)\n        self.linear_k = nn.Linear(embed_dim, embed_dim)\n        self.linear_v = nn.Linear(embed_dim, embed_dim)\n        self.linear_out = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, query, key, value):\n        batch_size = query.shape[0]\n\n        # Split the embedding into multiple heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n\n        # Compute attention scores\n        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n\n        # Apply softmax to obtain attention weights\n        attention_weights = F.softmax(attention_scores, dim=-1)\n\n        # Compute weighted sum of value vectors\n        attention_output = torch.matmul(attention_weights, value)\n\n        # Concatenate attention heads\n        attention_output = attention_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, embed_dim)\n\n        # Apply linear transformation for output\n        attention_output = self.linear_out(attention_output)\n\n        return attention_output, attention_weights\n</code></pre>"},{"location":"Computer%20Vision/Concepts/Attention/#cross-attention","title":"Cross Attention","text":"<p>Cross-attention, also known as encoder-decoder attention, is a type of attention mechanism used in the Transformer architecture for sequence-to-sequence tasks, such as machine translation. Unlike self-attention, which focuses on relationships within a single input sequence, cross-attention allows the decoder to attend to different parts of the encoder's input sequence when generating the output sequence.</p> <ul> <li> <p>Query, Key, and Value: In cross-attention, the query vectors are generated from the decoder's hidden states, while the key and value vectors are generated from the encoder's hidden states. This allows the decoder to attend to different parts of the encoder's input sequence when generating each token in the output sequence.</p> </li> <li> <p>Computing Attention Scores: The attention scores are computed as the dot product of the query vectors (decoder) with the key vectors (encoder). This measures the similarity between the decoder's current state and each token in the encoder's input sequence.</p> </li> <li> <p>Applying Softmax: Softmax is applied to the attention scores to obtain attention weights, which determine how much each token in the encoder's input sequence contributes to the generation of the current token in the decoder's output sequence.</p> </li> <li> <p>Weighted Sum: The final representation of each token in the decoder's output sequence is computed as a weighted sum of the encoder's value vectors, where the weights are the attention weights.</p> </li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CrossAttention(nn.Module):\n    def __init__(self, embed_dim):\n        super(CrossAttention, self).__init__()\n        self.query_linear = nn.Linear(embed_dim, embed_dim)\n        self.key_linear = nn.Linear(embed_dim, embed_dim)\n        self.value_linear = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, decoder_hidden, encoder_hidden):\n        # Compute query vectors from decoder's hidden states\n        query = self.query_linear(decoder_hidden)\n\n        # Compute key and value vectors from encoder's hidden states\n        key = self.key_linear(encoder_hidden)\n        value = self.value_linear(encoder_hidden)\n\n        # Compute attention scores\n        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n\n        # Apply softmax to obtain attention weights\n        attention_weights = F.softmax(attention_scores, dim=-1)\n\n        # Compute weighted sum of value vectors\n        context_vector = torch.matmul(attention_weights, value)\n\n        return context_vector, attention_weights\n</code></pre>"},{"location":"Computer%20Vision/Concepts/Attention/#conclusion","title":"Conclusion","text":"<p>These are some examples of attention mechanisms in neural networks. They allow models to focus on relevant parts of the input data and have been instrumental in achieving state-of-the-art performance in various machine learning tasks.</p>"},{"location":"Computer%20Vision/Concepts/Attention/#references","title":"References","text":"<ol> <li>Understanding Deep Learning</li> <li>Transformer Primer </li> </ol>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/","title":"CTC Decoding","text":"<p>CTC (Connectionist Temporal Classification) decoding is a technique used in sequence-to-sequence tasks, such as speech and optical character recognition (OCR), where the alignment between input and output sequences is not given during training. CTC loss is commonly employed to train models for such tasks. During decoding, the goal is to find the most likely output sequence given the model's predictions.</p>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#algorithm","title":"Algorithm","text":"<ol> <li>The input to CTC decoding is a sequence of probability distributions produced by the model for each time step. In the context of OCR, this would be the output of a character recognition model applied to an image.</li> <li>Convert the probabilities into a sequence of characters or symbols. This can be done by selecting the character with the highest probability at each time step.</li> <li>In the CTC context, it is common to introduce a special \"blank\" label denoted as \"-\", which represents gaps between characters. During decoding, duplicate characters and blank labels are typically removed or collapsed.</li> <li>Remove consecutive blank labels, leaving only a single blank between characters. This is done to ensure that adjacent characters are not merged into a single character.</li> <li>Merge repeated characters to obtain the final output sequence. This involves removing consecutive duplicate characters, leaving only one instance.</li> <li>The resulting sequence represents the decoded output. In OCR, this would be the recognized text.</li> </ol>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#decoding-types","title":"Decoding Types","text":"<p>During decoding, the goal is to transform the model's output (probability distributions over characters for each time step) into the final sequence of characters. Different decoding algorithms can be employed to achieve this. Here are some common types of decoding approaches in CTC</p>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#greedy-decoding","title":"Greedy Decoding","text":"<ul> <li>In Greedy Decoding, at each time step, the most likely character is selected as the output without considering future time steps.</li> <li>The output sequence is obtained by choosing the character with the highest probability at each time step.</li> <li>Greedy Decoding is simple but may not always yield the most accurate results, especially when there are uncertainties in the model's predictions.</li> </ul>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#beam-search","title":"Beam Search","text":"<ul> <li>Beam Search is a more advanced decoding algorithm that considers multiple hypotheses simultaneously.</li> <li>It maintains a set of candidate hypotheses (the beam) and expands them at each time step based on the model's probabilities.</li> <li>The width of the beam controls the number of candidate hypotheses considered, and it influences the trade-off between exploration and exploitation.</li> <li>Beam Search often produces better results than Greedy Decoding by exploring a broader range of possibilities.</li> </ul>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#prefix-beam-search","title":"Prefix Beam Search","text":"<ul> <li>Prefix Beam Search is an extension of Beam Search that considers the possibility of ending the sequence early.</li> <li>It introduces a stopping criterion, allowing the algorithm to consider partial sequences and their likelihood of being completed successfully.</li> <li>This can be particularly useful when the true sequence length is uncertain.</li> </ul>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#lexicon-constrained-decoding","title":"Lexicon-Constrained Decoding","text":"<ul> <li>Lexicon-Constrained Decoding involves incorporating a lexicon or dictionary of valid words during the decoding process.</li> <li>The decoder explores only those paths that correspond to valid words, reducing the search space and potentially improving accuracy.</li> </ul>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#word-beam-search","title":"Word Beam Search","text":"<ul> <li>Word Beam Search is an extension of Beam Search designed specifically for tasks where the output consists of complete words.</li> <li>It incorporates language models and considers word-level probabilities to guide the decoding process.</li> <li>Word Beam Search aims to produce coherent and meaningful output sequences.</li> </ul>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#best-path-decoding","title":"Best Path Decoding","text":"<ul> <li>Best Path Decoding is a simplified decoding strategy where the output sequence is obtained by selecting the most likely path through the model's probability distribution.</li> <li>This is equivalent to Greedy Decoding but may be used when computational resources are limited.</li> </ul>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#token-passing","title":"Token Passing","text":"<ul> <li>Token Passing is an efficient decoding algorithm that uses a set of active tokens to represent possible hypotheses.</li> <li>At each time step, the tokens are updated based on the model's probabilities.</li> <li>Token Passing is particularly useful when dealing with very long sequences as it allows for efficient pruning of unlikely paths.</li> </ul>"},{"location":"Computer%20Vision/Concepts/CTC%20Decoding/#conclusion","title":"Conclusion","text":"<p>The choice of decoding algorithm depends on the specific requirements of the task, the nature of the data, and available computational resources. Beam Search is a widely used and effective decoding strategy, but other methods may be more suitable in certain scenarios, such as when a lexicon is available or when dealing with word-level outputs.</p>"},{"location":"Computer%20Vision/Text%20Detection/craft/","title":"CRAFT","text":"<p>CRAFT Architecture for Text Detection Overview The CRAFT architecture is designed for precise text detection in images, with a specific focus on handling curved or irregularly shaped text. It employs a combination of a VGG-16 backbone, Feature Pyramid Network (FPN), and Text-Attentional Refinement Network (TARNet) to achieve accurate and detailed text region localization.</p> <p></p> <p>Components</p> <ol> <li> <p>Backbone Network CRAFT uses a VGG-16 architecture as its backbone network. This well-established architecture is known for its effectiveness in computer vision tasks.</p> </li> <li> <p>Feature Pyramid Network (FPN) FPN is integrated to enhance the model's ability to detect text at various scales, crucial for handling text of different sizes within an image.</p> </li> <li> <p>Text-Attentional Refinement Network (TARNet) TARNet refines initial predictions using pixel-wise binary classification and an attentional mechanism. It aids in improving the accuracy of text region boundaries.</p> </li> <li> <p>Bounding Box Prediction CRAFT predicts text regions at the pixel level, providing detailed information about the vertices of each text instance. Instead of predicting rectangular bounding boxes, it predicts the four vertices.</p> </li> <li> <p>Loss Function The loss function combines pixel-wise binary classification loss and regression loss for vertex prediction. This dual-loss structure optimizes the model for accurate text localization.</p> </li> <li> <p>Post-processing (Quad-NMS) A Quad-NMS post-processing step is applied to eliminate redundant or overlapping predictions. It helps enhance the precision of text detection by suppressing non-maximum predictions.</p> </li> <li> <p>Training Data CRAFT is trained on datasets with annotated text regions. The training process involves optimizing the model's parameters based on a combination of binary classification loss and regression loss.</p> </li> <li> <p>Usage Once trained, CRAFT can be employed for text detection in various applications, including document analysis, scene text recognition, and image-based information retrieval.</p> </li> </ol>"},{"location":"Computer%20Vision/Text%20Detection/dbnet/","title":"DBNet","text":""},{"location":"Computer%20Vision/Text%20Detection/dbnet/#introduction","title":"Introduction","text":"<p>Differentiable Binarization is an architecture used for text detection, particularly in the context of scene text recognition. It aims to address the challenges of accurately segmenting text regions from complex backgrounds in images.</p> <p>The architecture leverages a differentiable thresholding operation that allows for end-to-end training and gradient-based optimization. Traditional binarization methods use fixed thresholding techniques, which can be sensitive to variations in lighting, contrast, and background complexity. In contrast, the differentiable binarization architecture enables the model to learn an adaptive thresholding function during the training process.</p> <p>The key components of the differentiable binarization architecture include:</p> <p></p>"},{"location":"Computer%20Vision/Text%20Detection/dbnet/#backbone","title":"Backbone","text":"<p>The backbone network is typically a convolutional neural network (CNN) that processes the input image and learns high-level feature representations. It extracts hierarchical features from the image, capturing both local and global information.</p>"},{"location":"Computer%20Vision/Text%20Detection/dbnet/#binarization-module","title":"Binarization Module","text":"<p>Binarization module takes the feature maps generated by the backbone network and applies a sigmoid activation function to obtain a pixel-wise probability map. This probability map represents the probability of each pixel being part of a text region.</p> <p>Given a probability map \\( P \\in R^{H\u00d7W} \\) produced by a backbone network, it is essential to convert it to a binary map \\( P \\in R^{H\u00d7W} \\), where pixels with value 1 is considered as valid text areas. </p>"},{"location":"Computer%20Vision/Text%20Detection/dbnet/#differentiable-thresholding","title":"Differentiable Thresholding","text":"<p>Standard binarization module is not differentiable. So it cannot be optimized with along with the segmentation network during training. To solve this problem differentiable binarization was proposed. </p> <p>Differentiable binarization with adaptive thresholds can not only help differentiate text regions from the background, but also separate text instances which are closely aligned. </p> <p>Unlike traditional binarization methods that use a fixed threshold value to convert the probability map into a binary mask, differentiable binarization employs a differentiable thresholding operation. This operation uses a learned parameter to dynamically adjust the threshold during training. It allows the model to optimize the threshold value based on the loss function and backpropagate gradients through the thresholding process.</p>"},{"location":"Computer%20Vision/Text%20Detection/dbnet/#adaptive-threshold","title":"Adaptive Threshold","text":"<p>The threshold map would highlight the text border region even without supervision for the threshold map. This indicates that the border-like threshold map is beneficial to the final results. Thus, we apply borderlike supervision on the threshold map for better guidance.</p> <p></p>"},{"location":"Computer%20Vision/Text%20Detection/dbnet/#loss-function","title":"Loss Function","text":"<p>During training, the differentiable binarization architecture is optimized end-to-end by jointly learning the parameters of the backbone network and the binarization module. The gradients are backpropagated through the entire architecture, allowing the model to adaptively binarize the text regions in an image.</p> <p>The loss function L can be expressed as a weighted sum of the loss for the probability map \\( L_s\\), the loss for the binary map \\( L_b\\), and the loss for the threshold map \\( L_t\\)</p> \\[ L = L_s + \u03b1 \u00d7 L_b + \u03b2 \u00d7  L_t\\] <p>\\( \\alpha\\) and \\( \\beta \\) are set to 1.0 and 10 respectively. Binary cross entropy is applied for both \\( L_s\\) and \\( L_b\\). To overcome the unbalance of the number of positives and negatives, hard negative mining is used in the BCE loss by sampling the hard negatives. </p> <p>In the inference period, we can either use the probability map or the approximate binary map to generate text bounding boxes, which produces almost the same results.</p>"},{"location":"Computer%20Vision/Text%20Detection/dbnet/#box-formation","title":"Box formation","text":"<ul> <li>probability map is firstly binarized with a constant threshold (0.2), to get the binary map</li> <li>the connected regions (shrunk text regions) are obtained from the binary map</li> <li>the shrunk regions are dilated with an offset \\( D^{'}\\)</li> </ul>"},{"location":"Computer%20Vision/Text%20Recognition/vitstr/","title":"ViTSTR","text":"<p>Vision Transformer for Scene Text Recognition</p> <p></p>"},{"location":"Computer%20Vision/transformers/Data-efficient%20Image%20Transformer%20%28DeiT%29/","title":"Data efficient Image Transformer (DeiT)","text":"<p>The Data-efficient Image Transformer (DeiT) is a transformer-based architecture designed for image classification tasks. Introduced to address the challenge of achieving strong performance with limited labeled data, DeiT leverages distillation techniques and large-scale unlabeled datasets during training. Below is a simplified overview of the DeiT transformer architecture:</p>"},{"location":"Computer%20Vision/transformers/Data-efficient%20Image%20Transformer%20%28DeiT%29/#encoder","title":"Encoder","text":"<p>The encoder in the Transformer architecture is responsible for processing the input sequence, extracting relevant features, and creating a rich representation that can be used for downstream tasks such as classification or generation. The encoder consists of a stack of identical layers, each comprising two main sub-components: the self-attention mechanism and position-wise feedforward networks.</p> <p>Similar to other vision transformers, DeiT starts by dividing the input image into non-overlapping patches, which are linearly embedded to obtain a sequence of vectors. This process allows the model to process images as sequences of patches, making it compatible with the transformer architecture.</p>"},{"location":"Computer%20Vision/transformers/Data-efficient%20Image%20Transformer%20%28DeiT%29/#distillation","title":"Distillation","text":"<p>Distillation through attention, often referred to as attention distillation, is a technique used in machine learning to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). The primary goal is to improve the efficiency and computational performance of the smaller model while retaining its ability to make accurate predictions. This technique is particularly useful when deploying models to resource-constrained environments or edge devices.</p> <p>The attention mechanism in a transformer model plays a crucial role in capturing relationships between different parts of the input sequence. In attention distillation, the teacher model has a more sophisticated attention mechanism, and the goal is to transfer this knowledge to the student model's attention mechanism.</p> <p>Here's a simplified explanation of attention distillation:</p> <ol> <li> <p>Teacher Model: The teacher model is a larger and more complex model, often pre-trained on a large dataset. It has a highly expressive attention mechanism that allows it to capture intricate patterns and dependencies in the data.</p> </li> <li> <p>Student Model: The student model is a smaller and more computationally efficient model that aims to replicate the performance of the teacher model. However, due to its reduced size, it may not capture the same level of complexity.</p> </li> <li> <p>Distillation Process:</p> <ul> <li>Soft Labels: During training, instead of using hard labels (one-hot encoded vectors) for the ground truth, soft labels are generated based on the teacher model's predictions. Soft labels represent the probability distribution over classes for each input example.</li> <li>Attention Maps: Additionally, attention maps, which represent the distribution of attention weights assigned to different parts of the input sequence by the teacher model, are computed.</li> <li>Loss Function: The loss function for training the student model is a combination of the standard classification loss (e.g., cross-entropy loss) using soft labels and a distillation loss based on the attention maps. The distillation loss encourages the student model to mimic the attention patterns of the teacher model.</li> <li>Training Objective: The overall training objective is a weighted sum of the classification loss and the distillation loss. The distillation loss ensures that the student model not only predicts similar probabilities as the teacher model but also learns similar attention patterns.</li> </ul> </li> <li> <p>Inference: Once trained, the student model can be used for inference on new data. While it may have a smaller number of parameters, it benefits from the distilled knowledge of the teacher model, leading to improved generalization performance.</p> </li> </ol> <p></p>"},{"location":"Computer%20Vision/transformers/Data-efficient%20Image%20Transformer%20%28DeiT%29/#data-augmentation-and-training-strategies","title":"Data Augmentation and Training Strategies","text":"<p>DeiT benefits from extensive data augmentation techniques during training, helping the model learn robust features from diverse visual perspectives. Additionally, training on a large-scale dataset without labels aids the model in learning generic representations, contributing to its ability to perform well with smaller labeled datasets.</p> <p>Compared to models that integrate more priors (such as convolutions), transformers require a larger amount of data. Thus, in order to train with datasets of the same size, we rely on extensive data augmentation. We evaluate different types of strong data augmentation, with the objective to reach a data-efficient training regime.</p>"},{"location":"Computer%20Vision/transformers/Data-efficient%20Image%20Transformer%20%28DeiT%29/#sample-code","title":"** Sample Code**","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import models\n\nclass DistillationHead(nn.Module):\n    def __init__(self, in_features, num_classes):\n        super(DistillationHead, self).__init__()\n        self.fc = nn.Linear(in_features, num_classes)\n\n    def forward(self, x):\n        return self.fc(x)\n\nclass DeiTModel(nn.Module):\n    def __init__(self, model_name, num_classes, pretrained_teacher=None):\n        super(DeiTModel, self).__init__()\n\n        # Load pre-trained teacher model for distillation if provided\n        if pretrained_teacher is not None:\n            self.teacher = models.__dict__[pretrained_teacher](pretrained=True)\n            self.teacher_head = DistillationHead(self.teacher.fc.in_features, num_classes)\n        else:\n            self.teacher = None\n            self.teacher_head = None\n\n        # Load DeiT architecture\n        self.deit = models.vit_deit_small_patch16_224(pretrained=True)  # Example patch-size and image-size\n\n        # Classification head for DeiT\n        self.head = DistillationHead(self.deit.head.in_features, num_classes)\n\n    def forward(self, x):\n        # Forward pass through DeiT model\n        deit_features = self.deit(x)\n\n        # Forward pass through teacher model for distillation\n        if self.teacher is not None:\n            with torch.no_grad():\n                teacher_features = self.teacher(x)\n            teacher_logits = self.teacher_head(teacher_features)\n\n        # Forward pass through DeiT's classification head\n        deit_logits = self.head(deit_features)\n\n        if self.teacher is not None:\n            return deit_logits, teacher_logits\n        else:\n            return deit_logits\n\n# Example usage\nnum_classes = 1000\nmodel = DeiTModel(model_name='vit_deit_small_patch16_224', num_classes=num_classes)\n</code></pre>"},{"location":"Computer%20Vision/transformers/Data-efficient%20Image%20Transformer%20%28DeiT%29/#conclusion","title":"Conclusion","text":"<p>The use of distillation and efficient training strategies allows DeiT to achieve strong performance with fewer labeled examples compared to some traditional convolutional neural network architectures. It highlights the potential of leveraging unsupervised learning and knowledge transfer in improving the data efficiency of deep learning models for image classification tasks. Keep in mind that the specifics of the architecture may vary, and for a detailed understanding, it's recommended to refer to the official research paper or code documentation.</p>"},{"location":"Computer%20Vision/transformers/detr/","title":"Detection Transformer","text":"<p>Detection Transformer (DeTR) is a state-of-the-art object detection model that uses transformer architecture to directly predict object bounding boxes and categories in a single feedforward pass. The model was proposed in a paper titled \"End-to-End Object Detection with Transformers\" by Nicolas Carion et al. DeTR matches the performance of state-of-the-art methods  such as the Faster R-CNN baseline on the challenging COCO object detection dataset. </p>"},{"location":"Computer%20Vision/transformers/detr/#architecture","title":"Architecture","text":"<p>DeTR consists of two main components: a backbone network that encodes the input image and a transformer network that performs the object detection. Here's an overview of the DeTR architecture:</p> <p></p>"},{"location":"Computer%20Vision/transformers/detr/#cnn-backbone","title":"CNN Backbone","text":"<p>Convolutional neural network (CNN) encodes the input image into a sequence of feature maps. These feature maps are passed through a set of convolutional layers that gradually reduce the spatial resolution and increase the number of channels. Detr uses ResNet model that is pretrained on Imagenet. </p>"},{"location":"Computer%20Vision/transformers/detr/#encoder","title":"Encoder","text":"<ul> <li>\\(1x1\\) convolution is used to reduce the channel dimension of the high-level activation map f from C to a smaller dimension d.</li> <li>Since encoder expects sequence as in input, we collapse the spatial dimensions of \\(z_0\\) into one dimension, resulting in a d\u00d7HW feature map. </li> <li>Each encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network (FFN). </li> <li>Encoder outputs a set of learned features that are used to predict the bounding boxes and categories of the objects in the image.</li> </ul>"},{"location":"Computer%20Vision/transformers/detr/#decoder","title":"Decoder","text":"<ul> <li>Decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed self- and encoder-decoder attention mechanisms.</li> <li>Detr decodes the N objects in parallel at each decoder layer in contrast to standard transformers. </li> <li>These input embeddings are learnt positional encodings that we refer to as object queries. These are transformed into an output embedding by the decoder.</li> <li>Output embeddings are then independently decoded into box coordinates and class labels by a feed forward network resulting N final Predictions. </li> </ul>"},{"location":"Computer%20Vision/transformers/detr/#positional-encoding","title":"Positional Encoding","text":"<p>In order to incorporate spatial information into the transformer architecture, positional encodings are added to the learned features. The positional encodings are learned during training and encode the spatial information of the features.</p>"},{"location":"Computer%20Vision/transformers/detr/#object-queries","title":"Object Queries","text":"<p>The transformer network uses object queries to attend to specific regions in the feature maps and predict the corresponding bounding boxes and categories. The object queries are learnable embeddings that are passed through the transformer encoder along with the learned features.</p>"},{"location":"Computer%20Vision/transformers/detr/#prediction-heads","title":"Prediction Heads","text":"<p>The transformer network outputs a set of predictions for each object query. The predictions include the predicted class probability distribution, the predicted bounding box coordinates, and a binary indicator for whether an object is present or not.</p>"},{"location":"Computer%20Vision/transformers/detr/#ffn","title":"FFN","text":"<ul> <li>Final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d and a linear projection layer. </li> <li>FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image and the linear layer predicts the class label using a softmax function. </li> <li>We predict a fixed-size set of N bounding boxes, where N is usually much larger than the actual number of objects of interest in an image. </li> <li>\\(\\phi\\) is used to represent no object in an image (similar to background in object detection). </li> </ul>"},{"location":"Computer%20Vision/transformers/detr/#training-hyperparameters","title":"Training Hyperparameters","text":"<ul> <li>DeTR with AdamW setting the initial transformer\u2019s learning rate to \\(10^{-4}\\) , the backbone\u2019s to \\(10^{-5}\\) , and weight decay to \\(10^{-4}\\).</li> <li>scale augmentation - resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333. </li> <li>random crop augmentations during training, improving the per- formance by approximately 1 AP</li> <li>We use linear combination of l1 and GIoU losses for bounding box regression with \\(\\lambda_{L1}\\) = 5 and \\(\\lambda_{giou}\\) = 2 weights respectively. All models were trained with N = 100 decoder query slots. </li> <li>All losses are normalized by the number of objects inside the batch. </li> </ul>"},{"location":"Computer%20Vision/transformers/detr/#loss-function","title":"Loss Function","text":"<ul> <li>Loss function used during training is a combination of the binary cross-entropy loss for object presence, the smoothed L1 loss for bounding box regression, and the cross-entropy loss for classification. </li> <li> <p>Detr loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses. This is computed as follows</p> <ul> <li>computes an assignment between the targets and the predictions of the network</li> <li>Compute the classification cost</li> <li>Compute the L1 cost between boxes</li> <li>Compute the giou cost betwen boxes</li> <li>Final cost matrix =  \\(cost_{cls}\\) + \\(cost_{l1}\\) + \\(cost_{giou}\\)</li> </ul> </li> <li> <p>l1 loss will have different scales for small and large boxes even if their relative errors are similar. To mitigate this issue detr use a linear combination of the l1 loss and the generalized IoU loss. </p> </li> </ul>"},{"location":"Computer%20Vision/transformers/detr/#sample-code","title":"Sample Code","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet50\nfrom torch.nn import TransformerEncoder, TransformerDecoder\n\nclass DETR(nn.Module):\n    def __init__(self, num_classes, num_queries):\n        super(DETR, self).__init__()\n        # CNN backbone\n        self.backbone = resnet50(pretrained=True)\n        del self.backbone.fc  # remove the fully connected layer\n\n        # Transformer encoder\n        self.transformer_encoder = TransformerEncoder(...)\n\n        # Transformer decoder\n        self.transformer_decoder = TransformerDecoder(...)\n\n        # Final layers for class prediction and bounding box prediction\n        self.class_pred = nn.Linear(...)\n        self.bbox_pred = nn.Linear(...)\n\n        # Number of object queries\n        self.num_queries = num_queries\n\n    def forward(self, x):\n        # CNN backbone\n        features = self.backbone(x)\n\n        # Transformer encoder\n        enc_outputs = self.transformer_encoder(features)\n\n        # Initialize object queries\n        tgt = torch.zeros(self.num_queries, features.shape[0], features.shape[1])\n\n        # Transformer decoder\n        dec_outputs = self.transformer_decoder(tgt, enc_outputs)\n\n        # Final predictions\n        class_logits = self.class_pred(dec_outputs)\n        bbox_pred = self.bbox_pred(dec_outputs)\n\n        return class_logits, bbox_pred\n\n# Example usage:\n# Initialize DETR model\nnum_classes = 80  # COCO dataset has 80 object classes\nnum_queries = 100  # Number of object queries\nmodel = DETR(num_classes, num_queries)\n\n# Forward pass\ninput_image = torch.randn(1, 3, 224, 224)  # Example input image\nclass_logits, bbox_pred = model(input_image)\n\n# Output shapes: class_logits.shape = (num_queries, num_classes), bbox_pred.shape = (num_queries, 4)\n</code></pre>"},{"location":"Computer%20Vision/transformers/detr/#conclusion","title":"Conclusion","text":"<p>Overall, DeTR is a powerful object detection model that combines the strengths of both convolutional neural networks and transformer networks. By directly predicting the object bounding boxes and categories, DeTR eliminates the need for anchor boxes and achieves state-of-the-art performance on several object detection benchmarks.</p>"},{"location":"Computer%20Vision/transformers/detr/#references","title":"References","text":"<ol> <li>End-to-End Object Detection with Transformers</li> <li>Detection Transformer</li> </ol>"},{"location":"Computer%20Vision/transformers/swin/","title":"Swin Transformer","text":""},{"location":"Computer%20Vision/transformers/swin/#introduction","title":"Introduction","text":"<p>Swin Transformer is a recently proposed transformer architecture for image classification tasks. The model was proposed in a paper titled \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" by Ze Liu et al. Swin Transformer introduces a novel hierarchical architecture that divides the input image into a hierarchy of windows and processes them using a series of transformer blocks. Here's an overview of the Swin Transformer architecture:</p>"},{"location":"Computer%20Vision/transformers/swin/#architecture","title":"Architecture","text":""},{"location":"Computer%20Vision/transformers/swin/#patch-embeddings","title":"Patch Embeddings","text":"<p>First step in Swin Transformer is same as vision transformer. Convert an image into fixed patches of patch size 4x4. Then each patch is linearly embedded using a linear projection layer. This can be done using convolution 2d using kernel size and stride 4.  maps each window to a vector representation using a learnable embedding layer. The embeddings are then passed through a series of transformer blocks, which perform multi-head self-attention and feedforward operations.</p>"},{"location":"Computer%20Vision/transformers/swin/#patch-merging","title":"Patch Merging","text":"<p>The first patch merging layer concatenates the features of each group of 2\u00d72 neighboring patches, and applies a linear layer on the 4C - dimensional concatenated features. This reduces the number of tokens by a multiple of 2\u00d72 = 4 (2\u00d7 downsampling of resolution) and the output dimension is set to 2C. The number of tokens is reduced by patch merging layers as the network gets deeper. </p>"},{"location":"Computer%20Vision/transformers/swin/#shifted-window-attention","title":"Shifted Window Attention","text":"<p>At every stage in swin transformer, there are sequence of swin transformer blocks in tandem. Multi attention in  Standard transformer block is replaced using Shifted window self attention followed by 2 layered MLP and GELU. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module. </p> <p>The global computation leads to quadratic complexity with respect to the number of tokens, making it unsuitable for many vision problems. For efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.</p> <p>The window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.</p> <p>In first module feature map is evenly divided into small patches with local self attention. In second layer we displace the windows by (M/2, M/2) pixels from the regularly partitioned windows, and perform attention between these new windows. This leads to cross-window connections.  </p>"},{"location":"Computer%20Vision/transformers/swin/#classification-head","title":"Classification Head","text":"<p>Swin Transformer uses a simple linear layer to map the output embeddings to a set of class probabilities. The model is trained using a cross-entropy loss function, which measures the difference between the predicted probabilities and the true labels.</p>"},{"location":"Computer%20Vision/transformers/swin/#conclusion","title":"Conclusion","text":""},{"location":"Computer%20Vision/transformers/swin/#conclusion","title":"Conclusion","text":"<p>Overall, Swin Transformer is a powerful transformer architecture for image classification tasks that uses a novel hierarchical processing strategy and shifted windows to capture both local and global features. The model achieves state-of-the-art performance on several benchmark datasets, including ImageNet and COCO.</p>"},{"location":"Computer%20Vision/transformers/vit/","title":"Vision Transformer","text":""},{"location":"Computer%20Vision/transformers/vit/#introduction","title":"Introduction","text":"<p>The Vision Transformer (ViT) is a deep learning architecture introduced in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition\" by Alexey Dosovitskiy et al. The Vision Transformer applies the transformer architecture, originally designed for natural language processing tasks, to computer vision tasks such as image classification. Here's a detailed explanation of the key components and concepts of the Vision Transformer:</p>"},{"location":"Computer%20Vision/transformers/vit/#architecture","title":"Architecture","text":"<ol> <li>Image Patching:<ul> <li>The first step in the Vision Transformer is to break the input image into fixed-size non-overlapping patches. Each patch is treated as a token, and these patches are linearly embedded to obtain a sequence of vectors.</li> </ul> </li> <li>Patch Embedding:<ul> <li>A small convolutional neural network (CNN) is used as the patch embedding layer to convert each image patch into a fixed-size vector. This vector is then considered as the embedding for the corresponding image patch.</li> </ul> </li> <li>Positional Embeddings:<ul> <li>Similar to the original Transformer model, ViT uses positional embeddings to provide information about the spatial arrangement of tokens. These embeddings are added to the patch embeddings, allowing the model to understand the spatial relationships between different parts of the image.</li> </ul> </li> <li>Transformer Encoder:<ul> <li>The core of the Vision Transformer is the transformer encoder. It consists of multiple layers, each containing self-attention and feedforward sub-layers. The self-attention mechanism enables the model to capture global dependencies between patches, allowing it to consider the entire image context during classification.</li> </ul> </li> <li>Classification Token:<ul> <li>To perform classification, a special learnable token (CLS token) is prepended to the sequence of patch embeddings. The output of this token, after passing through the transformer layers, is used for making predictions. This design choice enables the model to capture global information for image classification.</li> </ul> </li> <li>Global Average Pooling:<ul> <li>Instead of relying on fully connected layers at the end of the network, ViT typically uses global average pooling after the transformer layers. This helps in reducing the number of parameters and promotes better generalization.</li> </ul> </li> </ol>"},{"location":"Computer%20Vision/transformers/vit/#training","title":"Training","text":"<p>ViT models are usually pre-trained on large datasets using a proxy task, such as image classification on a diverse set of images. The pre-trained model can then be fine-tuned on specific downstream tasks with smaller datasets.</p> <p>ViT can be pretrained with a masked patch prediction for self-supervision that is similar to masked language modeling in BERT. Refer this for more information on Masked Image Modeling. By pretraining on a large unlabeled dataset, the ViT can learn to generalize to a wide range of visual concepts and achieve better performance on tasks with limited labeled data.</p>"},{"location":"Computer%20Vision/transformers/vit/#advantages","title":"Advantages","text":"<p>The ViT architecture has been shown to achieve state-of-the-art performance on a range of image recognition tasks.  ViT has the advantage of being able to process images of arbitrary size, as it can be trained on images of different resolutions. ViT has the ability to handle long-range dependencies between different parts of an image. Additionally, ViT can be trained on large-scale datasets using self-supervised learning, which does not require expensive manual annotations.</p> <p>ViT demonstrates that the inductive biases introduced by CNNs are useful for small datasets, but not for larger ones. Experiments demonstrated that hard-coding the two-dimensional structure of the image patches into the positional encodings does not improve quality. </p>"},{"location":"Computer%20Vision/transformers/vit/#sample-code","title":"Sample Code","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom einops import rearrange\n\n# Vision Transformer Model\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10, embed_dim=256, num_heads=8, num_layers=6):\n        super(VisionTransformer, self).__init__()\n        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        num_patches = (img_size // patch_size) ** 2\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n        self.transformer = nn.Transformer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            num_encoder_layers=num_layers\n        )\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = rearrange(x, 'b c h w -&gt; b (h w) c')\n        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        x = x + self.pos_embed\n        x = self.transformer(x)\n        x = x.mean(dim=1)  # Global average pooling\n        x = self.fc(x)\n        return x\n\n# Data loading and preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32)),  # Adjust the size according to the model architecture\n])\n\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n\n# Initialize and train the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = VisionTransformer().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Note: This is a basic example, and for real-world scenarios, you might need to fine-tune the architecture and training parameters.\n</code></pre>"},{"location":"Computer%20Vision/transformers/vit/#conclusion","title":"Conclusion","text":"<p>In summary, the Vision Transformer is a groundbreaking architecture that applies the transformer model to image classification tasks. By representing images as sequences of tokens and leveraging self-attention mechanisms, ViT demonstrates competitive performance compared to traditional CNNs, especially on tasks involving complex visual relationships and large-scale datasets. </p>"},{"location":"Computer%20Vision/transformers/vit/#references","title":"References","text":"<ul> <li>AN IMAGE IS WORTH 16X16 WORDS</li> <li>Vision Transformer</li> <li>An Illustrative Guide to Masked Image Modelling</li> </ul>"},{"location":"Languages/python/Multiprocessing/","title":"Multiprocessing","text":"<p>The Python <code>multiprocessing</code> library is a module that supports the spawning of processes using a similar API to the threading module. It allows parallel execution of code on multiple processors or cores, taking advantage of multiple CPU cores to improve the performance of certain types of tasks. Here are the main components of the <code>multiprocessing</code> library</p>"},{"location":"Languages/python/Multiprocessing/#process","title":"Process","text":"<ul> <li>The <code>Process</code> class is the fundamental component of the <code>multiprocessing</code> library. It is used to create a new process.</li> <li>Instances of this class represent a separate process of execution. Each process has its own Python interpreter and memory space.</li> </ul> <pre><code>from multiprocessing import Process\n\ndef my_function():\n    print(\"Hello from a subprocess!\")\n\nif __name__ == \"__main__\":\n    my_process = Process(target=my_function)\n    my_process.start()\n    my_process.join()\n</code></pre>"},{"location":"Languages/python/Multiprocessing/#pool","title":"Pool","text":"<ul> <li>The <code>Pool</code> class provides a simple way to parallelize the execution of a function across multiple input values.</li> <li>It represents a pool of worker processes that can be used to parallelize the execution of a function across a set of input values.</li> <li><pre><code>from multiprocessing import Pool\n\ndef square(x):\n    return x * x\n\nif __name__ == \"__main__\":\n    with Pool(processes=4) as pool:\n        result = pool.map(square, [1, 2, 3, 4, 5])\n    print(result)\n</code></pre></li> </ul>"},{"location":"Languages/python/Multiprocessing/#queue","title":"Queue","text":"<ul> <li>The <code>Queue</code> class is used for communication between processes. It allows data to be exchanged between processes in a thread-safe manner.</li> <li>Processes can put items into the queue using the <code>put()</code> method and retrieve items using the <code>get()</code> method.</li> </ul> <pre><code>from multiprocessing import Process, Queue\n\ndef worker(q):\n    data = q.get()\n    print(f\"Worker received: {data}\")\n\nif __name__ == \"__main__\":\n    my_queue = Queue()\n    my_process = Process(target=worker, args=(my_queue,))\n    my_queue.put(\"Hello from the main process!\")\n    my_process.start()\n    my_process.join()\n</code></pre>"},{"location":"Languages/python/Multiprocessing/#lock","title":"Lock","text":"<ul> <li>The <code>Lock</code> class provides a way to synchronize access to shared resources. It allows only one process or thread to access the shared resource at a time.</li> </ul> <pre><code>from multiprocessing import Process, Lock\n\ndef update_shared_data(lock, shared_data):\n    with lock:\n        shared_data.value += 1\n\nif __name__ == \"__main__\":\n    from multiprocessing import Value\n\n    shared_data = Value(\"i\", 0)\n    my_lock = Lock()\n\n    processes = [Process(target=update_shared_data, args=(my_lock, shared_data)) for _ in range(5)]\n\n    for process in processes:\n        process.start()\n\n    for process in processes:\n        process.join()\n\n    print(\"Final value:\", shared_data.value)\n</code></pre>"},{"location":"Languages/python/Multiprocessing/#pipe","title":"Pipe","text":"<ul> <li>The <code>Pipe</code> class creates a two-way communication channel between two processes, allowing them to send and receive messages</li> </ul> <pre><code>from multiprocessing import Process, Pipe\n\ndef sender(conn):\n    conn.send(\"Hello from the sender!\")\n\ndef receiver(conn):\n    data = conn.recv()\n    print(f\"Receiver received: {data}\")\n\nif __name__ == \"__main__\":\n    parent_conn, child_conn = Pipe()\n    sender_process = Process(target=sender, args=(parent_conn,))\n    receiver_process = Process(target=receiver, args=(child_conn,))\n\n    sender_process.start()\n    receiver_process.start()\n\n    sender_process.join()\n    receiver_process.join()\n</code></pre>"},{"location":"Languages/python/Multiprocessing/#manager","title":"Manager","text":"<ul> <li> <p>The <code>Manager</code> class provides a way to create shared objects and data structures that can be accessed by multiple processes.</p> </li> <li> <p>It's useful for sharing more complex data structures like lists, dictionaries, and custom objects.</p> </li> </ul> <pre><code>from multiprocessing import Process, Manager\n\ndef update_shared_list(shared_list, index):\n    shared_list[index] = index * index\n\nif __name__ == \"__main__\":\n    with Manager() as manager:\n        my_list = manager.list([0, 0, 0, 0, 0])\n        processes = [Process(target=update_shared_list, args=(my_list, i)) for i in range(5)]\n\n        for process in processes:\n            process.start()\n\n        for process in processes:\n            process.join()\n\n        print(\"Updated shared list:\", my_list)\n</code></pre>"},{"location":"Languages/python/Multiprocessing/#event","title":"Event","text":"<ul> <li>The <code>Event</code> is a useful tool for coordinating activities among multiple processes in a multiprocessing environment.</li> <li>In the following example, <code>process1</code> waits for the event to be set, and <code>process2</code> sets the event after a delay. <pre><code>from multiprocessing import Process, Event\nimport time\n\ndef wait_for_event(event):\n    print(\"Waiting for event to be set.\")\n    event.wait()\n    print(\"Event has been set. Continuing.\")\n\ndef set_event(event, delay):\n    print(f\"Sleeping for {delay} seconds before setting the event.\")\n    time.sleep(delay)\n    event.set()\n    print(\"Event has been set.\")\n\nif __name__ == \"__main__\":\n    my_event = Event()\n\n    process1 = Process(target=wait_for_event, args=(my_event,))\n    process2 = Process(target=set_event, args=(my_event, 3))\n\n    process1.start()\n    process2.start()\n\n    process1.join()\n    process2.join()\n</code></pre></li> </ul>"},{"location":"Languages/python/Multiprocessing/#barrier","title":"Barrier","text":"<ul> <li><code>Barrier</code> class provides a way to synchronize multiple processes by blocking them until a specified number of processes have reached the barrier.</li> <li>Process barriers are useful in scenarios where multiple processes need to synchronize their progress, ensuring that they reach a common point before proceeding further</li> <li>In the following example, three worker processes are created, each waiting at the barrier. Once all three processes have reached the barrier, they proceed, and the main process prints a completion message <pre><code>from multiprocessing import Barrier, Process\nimport time\n\ndef worker(barrier):\n    print(f\"Worker {barrier.n_waiting + 1} is waiting at the barrier.\")\n    barrier.wait()\n    print(f\"Worker {barrier.n_waiting + 1} has passed the barrier.\")\n\nif __name__ == \"__main__\":\n    num_processes = 3\n    my_barrier = Barrier(num_processes)\n\n    processes = [Process(target=worker, args=(my_barrier,)) for _ in range(num_processes)]\n\n    for process in processes:\n        process.start()\n\n    for process in processes:\n        process.join()\n\n    print(\"Main process has completed.\")\n</code></pre></li> </ul>"},{"location":"Languages/python/Multiprocessing/#condition","title":"Condition","text":"<ul> <li>The <code>Condition</code> class provides a way for multiple processes to synchronize their execution based on a shared condition. It is similar to the <code>threading.Condition</code> class in the standard library but designed for use with multiprocessing.</li> </ul> <pre><code>from multiprocessing import Process, Condition\nimport time\n\ndef worker(condition):\n    with condition:\n        print(\"Worker waiting for a condition.\")\n        condition.wait()\n        print(\"Worker woke up!\")\n\ndef notifier(condition):\n    with condition:\n        print(\"Notifier notifying the condition.\")\n        condition.notify_all()\n\nif __name__ == \"__main__\":\n    my_lock = Lock()\n    my_condition = Condition(my_lock)\n\n    worker_process = Process(target=worker, args=(my_condition,))\n    notifier_process = Process(target=notifier, args=(my_condition,))\n\n    worker_process.start()\n\n    time.sleep(2)  # Simulate some work in the main process\n\n    notifier_process.start()\n    notifier_process.join()\n\n    with my_condition:\n        print(\"Main process notifying the condition.\")\n        my_condition.notify_all()\n\n    worker_process.join()\n</code></pre>"},{"location":"Languages/python/Multiprocessing/#references","title":"References","text":"<ol> <li>Multiprocessing documentation</li> <li> Python Multiprocessing: The Complete Guide</li> </ol>"},{"location":"Languages/python/pandas/","title":"Pandas","text":"<p>Pandas is a powerful and widely used data manipulation library in Python. It provides data structures for efficiently storing and manipulating large datasets. This tutorial will cover the basic functionalities of Pandas, including creating DataFrames, indexing, cleaning, and exploring data. </p>"},{"location":"Languages/python/pandas/#creating-dataframes","title":"Creating Dataframes","text":"<p>DataFrame is a two-dimensional, tabular data structure with labeled axes (rows and columns). It is one of the key data structures provided by Pandas and is widely used for data manipulation and analysis. Creating DataFrames can be done in various ways, such as from dictionaries, lists, CSV files, Excel files, and more. Here, we'll explore some common methods for creating DataFrames.</p> <ul> <li>From lists or arrays or list of dictionaries <pre><code>import pandas as pd\n\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': [25, 30, 22],\n        'City': ['New York', 'San Francisco', 'Los Angeles']}\n# or\ndata = [{'Name': 'Alice', 'Age': 25, 'City': 'New York'}, \n        {'Name': 'Bob', 'Age': 30, 'City': 'San Francisco'}, \n        {'Name': 'Charlie', 'Age': 22, 'City': 'Los Angeles'}]\n\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre></li> </ul> <p>This would result in a dataframe like this </p> <pre><code>      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   22    Los Angeles\n</code></pre> <ul> <li>From csvs <pre><code>df = pd.read_csv('data.csv')\n</code></pre></li> <li>From excel  <pre><code>df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n</code></pre></li> </ul>"},{"location":"Languages/python/pandas/#exploring-data","title":"Exploring data","text":"<p>Exploring data is a crucial step in any data analysis process, and Pandas provides a variety of functions to help you understand and analyze your dataset. Here are some common techniques for exploring data using Pandas in Python</p> <ul> <li>Basic <pre><code>import pandas as pd\n\n# Assuming df is your DataFrame\nprint(df.head())     # Display the first few rows\nprint(df.tail())     # Display the last few rows\nprint(df.info())     # Display concise summary\nprint(df.describe())  # Descriptive statistics\n</code></pre></li> <li>Indexing and selecting information <pre><code># Selecting a single column\nprint(df['Column'])\n\n# Selecting multiple columns\nprint(df[['Column1', 'Column2']])\n\n# Selecting a row by label\nprint(df.loc[0])\n\n# Selecting a row by integer index\nprint(df.iloc[0])\n\n# Selecting rows based on a condition\nprint(df[df['Column'] &gt; 25])\n</code></pre></li> <li>Missing values or duplicates <pre><code># Checking for missing values\nprint(df.isnull())\n\n# Dropping rows with any missing values\ndf_cleaned = df.dropna()\n\n# Filling missing values with a specific value\ndf_filled = df.fillna(value)\n\n# Checking for duplicate rows\nprint(df.duplicated())\n\n# Removing duplicate rows\ndf_no_duplicates = df.drop_duplicates()\n</code></pre></li> <li>creating new columns  <pre><code># Creating a new column based on existing columns\ndf['NewColumn'] = df['Column1'] + df['Column2']\n</code></pre></li> <li>Handling Categorical Data <pre><code># Converting a column to categorical\ndf['CategoryColumn'] = df['CategoryColumn'].astype('category')\n\n# Getting counts of each category\nprint(df['CategoryColumn'].value_counts())\n</code></pre></li> <li>Grouping <pre><code># Grouping by a column and applying aggregation functions\ngrouped_df = df.groupby('CategoryColumn').agg({'NumericColumn': ['mean', 'sum']})\nprint(grouped_df)\n</code></pre></li> <li>Visualization  <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Basic plotting\ndf.plot(x='Column1', y='Column2', kind='scatter')\nplt.show()\n\n# Using Seaborn for more advanced plots\nsns.boxplot(x='CategoryColumn', y='NumericColumn', data=df)\nplt.show()\n</code></pre></li> </ul>"},{"location":"Languages/python/pandas/#data-cleaning","title":"Data Cleaning","text":"<p>Data cleaning is an essential step in the data preparation process, ensuring that the dataset is accurate, consistent, and suitable for analysis. Pandas provides various functions to facilitate data cleaning tasks. Here are some common techniques for data cleaning using Pandas in Python</p> <ul> <li>Handling outliers  <pre><code>import numpy as np\n\nQ1 = np.percentile(df['NumericColumn'], 25)\nQ3 = np.percentile(df['NumericColumn'], 75)\nIQR = Q3 - Q1\n\n# Filtering data within 1.5 times the interquartile range\ndf_no_outliers = df[(df['NumericColumn'] &gt;= Q1 - 1.5 * IQR) &amp; (df['NumericColumn'] &lt;= Q3 + 1.5 * IQR)]\n</code></pre></li> <li>Correcting Data Types <pre><code>df['NumericColumn'] = pd.to_numeric(df['NumericColumn'])  # Convert column to numeric\ndf['DateColumn'] = pd.to_datetime(df['DateColumn'])      # Convert column to datetime\ndf['CategoryColumn'] = df['CategoryColumn'].astype('category')  # Convert column to category\n</code></pre></li> <li>Text data cleaning  <pre><code>df['TextColumn'] = df['TextColumn'].str.lower()      # Convert text to lowercase\ndf['TextColumn'] = df['TextColumn'].str.strip()      # Remove leading and trailing whitespaces\ndf['TextColumn'] = df['TextColumn'].str.replace('[^a-zA-Z ]', '')  # Remove non-alphabetic characters\n</code></pre></li> <li>Renaming columns <pre><code>df.rename(columns={'OldName': 'NewName'}, inplace=True)  # Rename a specific column\ndf.columns = ['NewColumn1', 'NewColumn2']  # Rename all columns\n</code></pre></li> <li> <p>Binarization  <pre><code>bins = [0, 25, 50, 75, 100]\nlabels = ['0-25', '26-50', '51-75', '76-100']\n\ndf['BinnedColumn'] = pd.cut(df['NumericColumn'], bins=bins, labels=labels)\n</code></pre></p> </li> <li> <p>Dealing with date and time  <pre><code>df['DateColumn'] = pd.to_datetime(df['DateColumn'])  # Convert column to datetime\ndf['DayOfWeek'] = df['DateColumn'].dt.day_name()  # Extract day of the week\ndf['Month'] = df['DateColumn'].dt.month  # Extract month\n</code></pre></p> </li> </ul>"},{"location":"Languages/python/pandas/#data-manipulation","title":"Data Manipulation","text":"<p>Data modification  involves making changes to the existing data, such as adding or removing columns, updating values, and creating new features. Here are some common data modification tasks using Pandas</p> <ul> <li>Adding or update an existing column <pre><code>import pandas as pd\n\n# Assuming df is your DataFrame\ndf['NewColumn'] = [1, 2, 3, 4, 5]\n\ndf['ExistingColumn'] = df['ExistingColumn'] * 2\n</code></pre></li> <li>Drop a column in place  <pre><code># Removing a single column\ndf.drop('ColumnToRemove', axis=1, inplace=True)\n\n# Removing multiple columns\ncolumns_to_remove = ['Column1', 'Column2']\ndf.drop(columns=columns_to_remove, inplace=True)\n</code></pre></li> <li>Applying functions  <pre><code># Applying a function to each element of a column\ndf['NumericColumn'] = df['NumericColumn'].apply(lambda x: x * 2)\n\n# Applying a function to each row\ndf['NewColumn'] = df.apply(lambda row: row['Column1'] + row['Column2'], axis=1)\n</code></pre></li> <li>Map and Reduce  <pre><code># Creating a new column based on a mapping\ngender_mapping = {'M': 'Male', 'F': 'Female'}\ndf['Gender'] = df['Code'].map(gender_mapping)\n</code></pre></li> <li>Change data type  <pre><code># Converting a column to numeric\ndf['NumericColumn'] = pd.to_numeric(df['NumericColumn'])\n\n# Converting a column to datetime\ndf['DateColumn'] = pd.to_datetime(df['DateColumn'])\n\n# Converting a column to category\ndf['CategoryColumn'] = df['CategoryColumn'].astype('category')\n</code></pre></li> <li>Sorting  <pre><code># Sorting based on one or more columns\ndf.sort_values(by=['Column1', 'Column2'], ascending=[True, False], inplace=True)\n</code></pre></li> <li>Combining data frames  <pre><code># Concatenating DataFrames along rows or columns\ndf_concatenated = pd.concat([df1, df2], axis=0)\n</code></pre></li> </ul>"},{"location":"Languages/python/pandas/#conclusion","title":"Conclusion","text":"<p>Pandas is an invaluable tool for anyone working with tabular data in Python. It provides a flexible and expressive framework for data manipulation, making it easier to clean, analyze, and visualize datasets. By mastering Pandas, you empower yourself to tackle a wide range of data-related tasks efficiently.</p>"},{"location":"Languages/python/pandas/#references","title":"References","text":"<ol> <li>Pandas documentation</li> </ol>"},{"location":"Languages/python/threading/","title":"Threading","text":"<p>The <code>threading</code> module in Python provides a way to create and manage threads, allowing you to write concurrent programs. Threads are lighter-weight than processes and share the same memory space, making them suitable for tasks that can be parallelized. Here's a brief introduction to the <code>threading</code> module:</p>"},{"location":"Languages/python/threading/#thread","title":"Thread","text":"<ul> <li>The <code>Thread</code> class is the core component of the <code>threading</code> module. You create threads by instantiating objects of this class.</li> <li>Example: <pre><code>import threading\ndef my_function():\n    # code to be executed in the thread\n\nmy_thread = threading.Thread(target=my_function)\n</code></pre></li> </ul>"},{"location":"Languages/python/threading/#thread-lifecycle","title":"Thread Lifecycle","text":"<p>The thread lifecycle in Python involves several states and transitions. Here's a detailed explanation of the different states a thread can go through in its lifecycle:</p> <ul> <li> <p>New State:</p> <ul> <li> <p>A thread is in the \"New\" state when it is created but not yet started. At this point, the thread has been instantiated but hasn't begun its execution.</p> <pre><code>import threading\n\ndef my_function():\n    # Code to be executed in the thread\n\nmy_thread = threading.Thread(target=my_function)  # Thread is in the \"New\" state\n</code></pre> </li> </ul> </li> <li> <p>Runnable/Ready State:</p> <ul> <li> <p>After a thread is created, it enters the \"Runnable\" or \"Ready\" state when the <code>start()</code> method is called. In this state, the thread is ready to run but may not have been scheduled by the operating system yet.</p> <pre><code> my_thread.start()  # Thread is in the \"Runnable\" state\n</code></pre> </li> </ul> </li> <li> <p>Running State:</p> <ul> <li>When the operating system scheduler assigns CPU time to the thread, it enters the \"Running\" state. In this state, the thread's <code>run()</code> method is being executed.<pre><code>```\n# Thread is in the \"Running\" state (while executing the run() method)\n```\n</code></pre> </li> </ul> </li> <li> <p>Blocked/Waiting State:</p> <ul> <li> <p>A thread can move from the \"Running\" state to the \"Blocked\" or \"Waiting\" state when it is waiting for a resource, input, or some condition to be satisfied. It voluntarily releases the CPU.     ``` py     import threading     import time</p> <p>def my_function():     with some_lock:         time.sleep(5)  # Thread is in the \"Blocked\" state, waiting for some_lock</p> </li> </ul> </li> <li> <p>Terminated State:</p> <ul> <li> <p>A thread enters the \"Terminated\" state when its <code>run()</code> method completes or when an unhandled exception is raised within the thread. Once terminated, a thread cannot be restarted.</p> <pre><code>my_thread.join()\n</code></pre> </li> </ul> </li> </ul> <p>It's important to note that the GIL (Global Interpreter Lock) in CPython impacts the behavior of threads, especially in CPU-bound tasks. The GIL allows only one thread to execute Python bytecode at a time, limiting the effectiveness of threads for parallelizing certain types of operations. For CPU-bound tasks, multiprocessing may be a more suitable alternative.     - </p>"},{"location":"Languages/python/threading/#thread-synchronization","title":"Thread Synchronization:","text":"<p>Thread synchronization in Python is essential for managing access to shared resources and preventing data corruption in a multithreaded environment. The <code>threading</code> module provides several synchronization primitives to facilitate this. Here are some key mechanisms</p> <ul> <li> <p>Locks</p> <ul> <li>A Lock is the simplest synchronization primitive. It is used to enforce exclusive access to a shared resource.</li> <li>Threads can acquire a lock using <code>acquire()</code> and release it using <code>release()</code>.</li> <li>Example:      <pre><code>import threading\nshared_resource = 0\nlock = threading.Lock()\n\ndef increment():\n    global shared_resource\n    with lock:\n        shared_resource += 1\n</code></pre></li> </ul> </li> <li> <p>Semaphores:</p> <ul> <li>Semaphores are counters that control access to a resource. They are often used to limit the number of threads that can access a resource simultaneously.</li> <li>The <code>Semaphore</code> class in the <code>threading</code> module provides this functionality.</li> <li>Example:     <pre><code>import threading\nshared_resource = 0\nsemaphore = threading.Semaphore(value=3)\ndef increment():\n    global shared_resource\n    with semaphore:\n        shared_resource += 1\n</code></pre></li> </ul> </li> <li> <p>Events </p> <ul> <li>An <code>Event</code> is a simple signaling mechanism that allows one thread to notify others about a certain condition.</li> <li>Threads can wait for an event using <code>wait()</code> and set the event using <code>set()</code>.</li> <li>Example:     <pre><code> import threading\nshared_condition = threading.Event()\n\ndef wait_for_event():\n    shared_condition.wait()\n    # Continue with the task after the event is set\n\ndef set_event():\n    shared_condition.set()\n</code></pre></li> </ul> </li> <li> <p>Conditions </p> <ul> <li>A <code>Condition</code> is more advanced than an event and is often used to coordinate threads based on shared state.</li> <li>It combines a lock and a signaling mechanism.</li> <li>Example:     <pre><code>import threading\n\nshared_resource = 0\ncondition = threading.Condition()\n\ndef modify_resource():\n    global shared_resource\n    with condition:\n        # Modify shared resource\n        shared_resource += 1\n        # Notify waiting threads\n        condition.notify_all()\n\ndef wait_for_change():\n    with condition:\n        while shared_resource == 0:\n            condition.wait()\n        # Continue the task after shared resource changes\n</code></pre></li> </ul> </li> </ul> <p>These synchronization primitives help ensure that critical sections of code are executed atomically and that threads cooperate properly when accessing shared data. The choice of synchronization mechanism depends on the specific requirements of your multithreaded application.</p>"},{"location":"Languages/python/threading/#thread-pool","title":"Thread pool","text":"<p>Thread pools in Python provide a way to efficiently manage and reuse a fixed number of threads for executing tasks concurrently. The <code>concurrent.futures</code> module, introduced in Python 3.2, offers a high-level interface for working with thread pools through the <code>ThreadPoolExecutor</code> class. Here's a detailed explanation</p> <ul> <li> <p>ThreadPoolExecutor:</p> <ul> <li>The <code>ThreadPoolExecutor</code> class is part of the <code>concurrent.futures</code> module.</li> <li>It provides a simple and consistent interface for working with thread pools.</li> <li> <p>To create a thread pool, you instantiate <code>ThreadPoolExecutor</code> with the desired number of worker threads</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    # Execute tasks using the executor\n</code></pre> </li> </ul> </li> <li> <p>submit method:</p> <ul> <li>The primary method of <code>ThreadPoolExecutor</code> is <code>submit(func, *args, **kwargs)</code>.</li> <li>It schedules the given callable (function or method) to be executed asynchronously by a thread in the pool.</li> <li>The method returns a <code>concurrent.futures.Future</code> object representing the result of the computation.     <pre><code>future = executor.submit(my_function, arg1, arg2)\n</code></pre></li> </ul> </li> <li> <p>map method:</p> <ul> <li>The <code>map(func, *iterables, timeout=None)</code> method can be used to parallelize the execution of a function over multiple input values.</li> <li>It's similar to the built-in <code>map</code> function but executes the function concurrently using the thread pool.     <pre><code>results = executor.map(my_function, iterable_of_args)\n</code></pre></li> </ul> </li> <li> <p>shutdown method:</p> <ul> <li> <p>To gracefully shut down the thread pool, you can use the <code>shutdown(wait=True)</code> method. The <code>wait</code> parameter specifies whether to wait for all submitted tasks to complete before shutting down.</p> <pre><code>executor.shutdown(wait=True)\n</code></pre> </li> </ul> </li> <li> <p>handling Results </p> <ul> <li> <p>The <code>concurrent.futures.Future</code> objects returned by <code>submit</code> can be used to retrieve the result of a computation.</p> </li> <li> <p>You can check if a future is done using the <code>done()</code> method and retrieve the result using the <code>result()</code> method.</p> <pre><code>if future.done():\n    result = future.result()\n</code></pre> </li> </ul> </li> <li> <p>Exception Handling</p> <ul> <li>Exceptions raised in a thread are captured and re-raised when calling <code>result()</code> on the corresponding <code>Future</code> object.</li> <li>It allows you to handle exceptions that occurred during the execution of a task     <pre><code>try:\n    result = future.result()\nexcept Exception as e:\n    # Handle the exception\n</code></pre></li> </ul> </li> </ul> <p>Thread pools are particularly useful for parallelizing I/O-bound tasks where threads can be efficiently reused. However, for CPU-bound tasks, consider using the <code>ProcessPoolExecutor</code> or other multiprocessing approaches due to the Global Interpreter Lock (GIL) in CPython.</p>"},{"location":"Languages/python/threading/#daemon-threads","title":"Daemon Threads","text":"<p>A daemon thread is a thread that runs in the background and is subordinate to the main program. Daemon threads are typically used for tasks that don't need to be explicitly waited for or completed before the program exits. When the main program exits, any remaining daemon threads are abruptly terminated, regardless of whether they have finished their tasks or not.</p> <ul> <li> <p>Creating Daemon Threads:</p> <ul> <li>You can mark a thread as a daemon thread by setting its <code>daemon</code> attribute to <code>True</code> before starting it. The <code>setDaemon(True)</code> method can also be used.     <pre><code>import threading\nimport time\n\ndef daemon_function():\n    while True:\n        print(\"Daemon Thread is running...\")\n        time.sleep(1)\n\ndaemon_thread = threading.Thread(target=daemon_function)\ndaemon_thread.setDaemon(True)  # Alternatively: daemon_thread.daemon = True\ndaemon_thread.start()\n</code></pre></li> </ul> </li> <li> <p>Daemon vs. Non-Daemon Threads:</p> <ul> <li>Non-daemon threads are considered foreground threads. The main program will wait for them to complete before it exits.</li> <li>Daemon threads, on the other hand, are background threads. They do not prevent the main program from exiting, and if any daemon threads are still running when the program exits, they are abruptly terminated.</li> </ul> </li> <li>Use Cases for Daemon Threads:<ul> <li>Daemon threads are suitable for background tasks such as monitoring, periodic cleanup, or tasks that should run indefinitely while the main program is running.</li> <li>They are not suitable for tasks that need to be completed before the program exits since they might be terminated abruptly.</li> </ul> </li> <li>Termination of Daemon Threads:<ul> <li>Daemon threads are terminated when the main program exits. This termination can be abrupt, potentially leading to incomplete tasks or resource leaks.</li> <li>It's essential to ensure that daemon threads perform tasks that can be safely terminated at any point without causing issues.</li> </ul> </li> <li>Joining Daemon Threads:<ul> <li>While daemon threads do not prevent the main program from exiting, you can still use the <code>join()</code> method to wait for a daemon thread to finish its current task before proceeding.     <pre><code>daemon_thread.join()\n</code></pre></li> </ul> </li> <li> <p>Default Daemon Status:</p> <ul> <li>If you don't explicitly set the <code>daemon</code> attribute, threads are non-daemon by default.     <pre><code>default_thread = threading.Thread(target=some_function)  # This thread is non-daemon\n</code></pre></li> </ul> </li> </ul> <p>Thread-local data is a mechanism that allows each thread to have its own instance of a variable, making it unique to that thread. This is particularly useful when working with threads that share the same global variables, as it prevents interference and data corruption between threads. The <code>threading</code> module provides the <code>local()</code> function to create thread-local data.</p> <ul> <li>The <code>local()</code> function creates an instance of a thread-local storage object. This object can hold variables that are unique to each thread.     <pre><code>import threading\nlocal_data = threading.local()\n</code></pre></li> <li>You can then assign variables to the thread-local object, and each thread will have its own independent copy of the variable     <pre><code>local_data.variable = 42\n</code></pre></li> <li>To access the thread-local data from within a thread, you simply use the thread-local object.     <pre><code>def my_function():\n    print(local_data.variable)\n\nmy_thread = threading.Thread(target=my_function)\nmy_thread.start()\n</code></pre></li> <li>By default, thread-local data is cleaned up when the thread exits. However, you can explicitly clean it up using the <code>del</code> statement.     <pre><code>del local_data\n</code></pre></li> <li>Uses<ul> <li>Thread-local data is often used when multiple threads need to work with shared resources, but each thread needs its own independent view of those resources.</li> <li>It's beneficial in scenarios where global variables might be accessed and modified by multiple threads, and you want to avoid data corruption and race conditions <pre><code>import threading\n\nglobal_variable = 0\nlock = threading.Lock()\n\ndef increment_global():\n    global global_variable\n    with lock:\n        global_variable += 1\n        print(f\"Global variable value: {global_variable}\")\n\ndef thread_function():\n    local_data.local_variable = 100\n    increment_global()\n\nlocal_data = threading.local()\n\nmy_thread = threading.Thread(target=thread_function)\nmy_thread.start()\n</code></pre></li> </ul> </li> </ul>"},{"location":"Languages/python/threading/#thread-local","title":"Thread local","text":""},{"location":"Languages/python/threading/#conclusion","title":"Conclusion","text":"<p>In summary, the <code>threading</code> module provides a convenient way to work with threads in Python. It allows you to create, start, and manage threads, and includes features for synchronization and coordination between threads. Understanding these basics is crucial for developing concurrent and parallel programs in Python.</p>"},{"location":"Languages/python/threading/#references","title":"References","text":"<ol> <li>Threading documentation</li> <li>The threading module</li> </ol>"},{"location":"MLOps/docker/","title":"Docker","text":""},{"location":"MLOps/docker/#docker","title":"Docker","text":"<p>Docker is a containerization platform that enables the development, deployment, and scaling of applications in isolated, lightweight containers. The key components of Docker include:</p>"},{"location":"MLOps/docker/#docker-engine","title":"Docker Engine","text":"<p>The core of Docker that runs and manages containers. This Consists of a daemon process (dockerd) and a command-line interface (CLI) tool (docker).</p> <ul> <li><code>docker --version</code>: Display Docker version.</li> <li><code>docker info</code>: Display system-wide information.</li> <li><code>docker help</code>: Display help information.</li> </ul>"},{"location":"MLOps/docker/#images","title":"Images","text":"<p>A lightweight, standalone, and executable package that includes everything needed to run a piece of software, such as the code, runtime, libraries, and system tools. Images are built from a Dockerfile and can be shared and versioned through Docker registries.</p> <ul> <li><code>docker images</code>: List all images.</li> <li><code>docker pull &lt;image&gt;</code>: Download an image from a repository.</li> <li><code>docker build -t &lt;tag&gt; .</code>: Build an image from the current directory's Dockerfile.</li> </ul>"},{"location":"MLOps/docker/#containers","title":"Containers","text":"<p>An instance of a Docker image that runs as a process on the host machine. Containers are isolated from each other and the host system, providing consistency across different environments.</p> <ul> <li><code>docker ps</code>: List running containers.</li> <li><code>docker ps -a</code>: List all containers (running and stopped).</li> <li><code>docker run &lt;image&gt;</code>: Create and start a container from an image.</li> <li><code>docker exec -it &lt;container&gt; &lt;command&gt;</code>: Execute a command in a running container.</li> </ul>"},{"location":"MLOps/docker/#networking","title":"Networking","text":"<p>Enables communication between Docker containers and between containers and the host system.  Docker provides various network drivers, and users can create custom networks to isolate and control traffic.</p> <ul> <li><code>docker network ls</code>: List all networks.</li> <li><code>docker network create &lt;name&gt;</code>: Create a network.</li> <li><code>docker run --network=&lt;network&gt; &lt;image&gt;</code>: Run a container in a specific network.</li> </ul>"},{"location":"MLOps/docker/#volumes","title":"Volumes","text":"<p>A persistent data storage mechanism in Docker. Volumes can be used to share data between containers, persist data beyond the lifecycle of a container, and facilitate easy data backup and restore. </p> <ul> <li><code>docker volume ls</code>: List all volumes.</li> <li><code>docker volume create &lt;name&gt;</code>: Create a volume.</li> <li><code>docker run -v &lt;volume&gt;:&lt;container_path&gt; &lt;image&gt;</code>: Mount a volume to a container.</li> </ul>"},{"location":"MLOps/docker/#compose","title":"Compose","text":"<p>A tool for defining and managing multi-container Docker applications using a YAML file (<code>docker-compose.yml</code>). Allows you to define the services, networks, and volumes required for an application, making it easier to manage complex deployments.</p> <ul> <li><code>docker-compose up</code>: Start services defined in <code>docker-compose.yml</code>.</li> <li><code>docker-compose down</code>: Stop and remove containers defined in <code>docker-compose.yml</code>.</li> </ul>"},{"location":"MLOps/docker/#dockerfile-directives","title":"Dockerfile Directives","text":"<p>A text file containing instructions for building a Docker image.  Defines the base image, adds application code, specifies dependencies, and configures runtime settings. - <code>FROM</code>: Base image for building. - <code>COPY</code>: Copy files or directories into the image. - <code>RUN</code>: Execute commands in the image. - <code>EXPOSE</code>: Specify ports to expose. - <code>CMD</code>: Default command to run when the container starts.</p>"},{"location":"MLOps/docker/#docker-registry","title":"Docker Registry","text":"<p>A centralized repository for storing and sharing Docker images. Examples include Docker Hub, a public registry, and private registries that organizations set up for their specific needs.</p> <ul> <li><code>docker login</code>: Log in to a Docker registry.</li> <li><code>docker push &lt;image&gt;</code>: Push an image to a registry.</li> <li><code>docker pull &lt;registry&gt;/&lt;image&gt;</code>: Pull an image from a registry.</li> </ul>"},{"location":"MLOps/docker/#cleanup","title":"Cleanup","text":"<ul> <li><code>docker system prune</code>: Remove all stopped containers, unused networks, and dangling images.</li> <li><code>docker volume prune</code>: Remove all unused volumes.</li> </ul>"},{"location":"MLOps/docker/#docker-hub","title":"Docker Hub","text":"<p>A cloud-based registry service provided by Docker for sharing and managing Docker images. Developers can push and pull images to and from Docker Hub, making it a central hub for the Docker community.</p>"},{"location":"MLOps/docker/#references","title":"References","text":"<ol> <li>Docker cheatsheet</li> <li>The Ultimate Docker Cheat Sheet</li> </ol>"},{"location":"MLOps/kubernetes/","title":"Kubernetes","text":"<p>Kubernetes, often abbreviated as K8s, is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. Originally developed by Google and later donated to the Cloud Native Computing Foundation (CNCF), Kubernetes has become a standard for deploying and managing applications in containers.</p>"},{"location":"MLOps/kubernetes/#key-kubernetes-concepts","title":"Key Kubernetes Concepts","text":"<ul> <li>Nodes: The physical or virtual machines that run your containers. Each node must have a container runtime (like Docker) installed.</li> <li>Pods: The smallest deployable units in Kubernetes. Pods can contain one or more containers that share network and storage resources.</li> <li>Services: An abstraction to expose a group of pods as a network service. Services enable communication between different parts of an application.</li> <li>ReplicaSets: Ensure a specified number of pod replicas are running at all times. It's a way to achieve high availability and scalability.</li> <li>Deployments: Provide declarative updates to applications. They describe the desired state for applications and manage the deployment process to achieve that state.</li> <li>ConfigMaps and Secrets: Store configuration information and sensitive data separately from the application code.</li> </ul>"},{"location":"MLOps/kubernetes/#pods","title":"Pods","text":"<p>Pod is the smallest deployable unit and the basic building block for running applications. A Pod represents a single instance of a running process in a cluster and encapsulates one or more containers. Containers within a Pod share the same network namespace, allowing them to communicate with each other using <code>localhost</code>.</p> <ul> <li> <p>Create a Pod</p> <p><code>kubectl create pod &lt;pod-name&gt; --image=&lt;container-image&gt;</code></p> </li> <li> <p>List Pods</p> <p><code>kubectl get pods</code></p> </li> <li> <p>Describe a Pod</p> <p><code>kubectl describe pod &lt;pod-name&gt;</code></p> </li> <li> <p>Delete a Pod</p> <p><code>kubectl delete pod &lt;pod-name&gt;</code></p> </li> <li> <p>Access Pod Terminal</p> <p><code>kubectl exec -it &lt;pod-name&gt; -- /bin/bash</code></p> </li> <li> <p>View Pod Logs</p> <p><code>kubectl logs &lt;pod-name&gt;</code></p> </li> </ul>"},{"location":"MLOps/kubernetes/#deployments","title":"Deployments","text":"<p>Deployment is an abstraction that provides declarative updates to applications. It allows you to describe the desired state for your application, and Kubernetes will then work to ensure that the current state of the deployed application matches the specified desired state. Deployments are a higher-level abstraction compared to managing individual Pods, providing features like scaling, rolling updates, and self-healing.</p> <ul> <li> <p>Create a Deployment</p> <p><code>kubectl create deployment &lt;deployment-name&gt; --image=&lt;container-image&gt;</code></p> </li> <li> <p>List Deployments</p> <p><code>kubectl get deployments</code></p> </li> <li> <p>Scale a Deployment</p> <p><code>kubectl scale deployment &lt;deployment-name&gt; --replicas=&lt;num-replicas&gt;</code></p> </li> </ul>"},{"location":"MLOps/kubernetes/#services","title":"Services","text":"<p>Service is an abstraction that defines a logical set of Pods and a policy by which to access them. Services allow applications to communicate with each other or with external clients, abstracting away the details of the underlying network. </p> <ul> <li> <p>Expose a Deployment as a Service</p> <p><code>kubectl expose deployment &lt;deployment-name&gt; --port=&lt;port&gt; --target-port=&lt;target-port&gt; --type=NodePort</code></p> </li> <li> <p>List Services</p> <p><code>kubectl get services</code></p> </li> <li> <p>Describe a Service</p> <p><code>kubectl describe service &lt;service-name&gt;</code></p> </li> </ul>"},{"location":"MLOps/kubernetes/#configmaps","title":"ConfigMaps","text":"<p>ConfigMaps provide a way to decouple configuration data from the application code. They allow you to manage configuration settings for your applications separately from the application code, making it easier to update configurations without modifying the application itself. ConfigMaps are particularly useful for applications that need configuration information such as environment variables, command-line arguments, or configuration files. </p> <ul> <li> <p>Create a ConfigMap</p> <p><code>kubectl create configmap &lt;configmap-name&gt; --from-literal=&lt;key1&gt;=&lt;value1&gt; --from-literal=&lt;key2&gt;=&lt;value2&gt;</code></p> </li> <li> <p>List ConfigMaps</p> <p><code>kubectl get configmaps</code></p> </li> </ul>"},{"location":"MLOps/kubernetes/#secrets","title":"Secrets","text":"<p>Secrets are used to store sensitive information, such as passwords, API keys, and other confidential data. Like ConfigMaps, Secrets allow you to decouple sensitive information from your application code, enhancing security and making it easier to manage and update credentials without modifying the application itself.</p> <ul> <li> <p>Create a Secret</p> <p><code>kubectl create secret generic &lt;secret-name&gt; --from-literal=&lt;key1&gt;=&lt;value1&gt; --from-literal=&lt;key2&gt;=&lt;value2&gt;</code></p> </li> <li> <p>List Secrets</p> <p><code>kubectl get secrets</code></p> </li> </ul>"},{"location":"MLOps/kubernetes/#namespaces","title":"Namespaces","text":"<p>Namespace is a way to organize and segregate resources within a cluster. It provides a scope for names, avoiding naming collisions between resources in different namespaces. Namespaces can be particularly useful in large, multi-tenant clusters where multiple teams or applications share the same Kubernetes cluster.</p> <ul> <li> <p>Create a Namespace</p> <p><code>kubectl create namespace &lt;namespace-name&gt;</code></p> </li> <li> <p>List Namespaces</p> <p><code>kubectl get namespaces</code></p> </li> </ul>"},{"location":"MLOps/kubernetes/#context","title":"Context","text":"<p>Context is a set of access parameters for a Kubernetes cluster. It includes information such as the cluster endpoint, the user's credentials, and the namespace. Contexts allow users to switch between different clusters and configurations easily. The <code>kubectl</code> command-line tool uses contexts to determine which cluster it should interact with.</p> <ul> <li> <p>Switch Context</p> <p><code>kubectl config use-context &lt;context-name&gt;</code></p> </li> </ul>"},{"location":"MLOps/kubernetes/#upgrades-and-rollbacks","title":"Upgrades and Rollbacks","text":"<p>Upgrades and rollbacks are crucial operations to maintain the health and reliability of your cluster. Upgrading Kubernetes involves updating the control plane components, such as the API server, controller manager, scheduler, and others. Rollbacks, on the other hand, allow you to revert to a previous version of the control plane in case of issues or unexpected behavior.</p> <ul> <li> <p>Update container image</p> <p><code>kubectl set image deployment/&lt;deployment_name&gt; &lt;container_name&gt;=&lt;new_image&gt;</code> .</p> </li> <li> <p>View rollout history.</p> <p><code>kubectl rollout history deployment/&lt;name&gt;</code> </p> </li> <li> <p>Rollback to previous deployment.</p> <p><code>kubectl rollout undo deployment/&lt;name&gt;</code> </p> </li> </ul>"},{"location":"MLOps/kubernetes/#monitoring-and-debugging","title":"Monitoring and Debugging","text":"<p>Monitoring and debugging are critical aspects of managing a Kubernetes cluster. They help ensure the health, performance, and reliability of applications running within the cluster. Kubernetes provides various tools and approaches for monitoring and debugging.</p> <ul> <li> <p>Display resource usage.</p> <p><code>kubectl top nodes/pods</code> </p> </li> <li> <p>Describe a resource for troubleshooting.</p> <p><code>kubectl describe &lt;resource_type&gt; &lt;resource_name&gt;</code> </p> </li> </ul>"},{"location":"MLOps/kubernetes/#persistent-volumes-pv","title":"Persistent Volumes (PV)","text":"<p>Persistent Volumes (PVs) are a way to provide durable storage for applications. PVs are part of the cluster's storage infrastructure, and they represent physical or networked storage resources in the cluster. They decouple storage provisioning from Pod lifecycle, allowing for the persistence of data even when Pods are rescheduled or recreated.</p> <ul> <li> <p>List persistent volumes.   </p> <p><code>kubectl get pv</code> </p> </li> <li> <p>Describe a persistent volume.</p> <p><code>kubectl describe pv/&lt;pv_name&gt;</code> </p> </li> </ul>"},{"location":"MLOps/kubernetes/#custom-resources","title":"Custom Resources","text":"<p>Custom Resources (CRs) and Custom Resource Definitions (CRDs) provide a way to extend the Kubernetes API and introduce custom objects with specific behaviors and semantics. Custom Resources allow users to define and manage application-specific resources beyond the built-in types like Pods, Services, and Deployments.</p> <ul> <li> <p>List Custom Resource Definitions.</p> <p><code>kubectl get crd</code> </p> </li> <li> <p>List instances of a custom resource.</p> <p><code>kubectl get &lt;custom_resource&gt;</code> </p> </li> <li> <p>Describe a custom resource instance.</p> <p><code>kubectl describe &lt;custom_resource&gt; &lt;name&gt;</code> </p> </li> </ul>"},{"location":"MLOps/kubernetes/#references","title":"References","text":"<ol> <li> kubectl Quick Reference</li> <li>kubectl cheatsheet</li> </ol>"},{"location":"NLP/Courses/stanford/","title":"Standford","text":""},{"location":"NLP/Courses/stanford/#cs-224n-natural-language-processing-with-deep-learning","title":"CS 224N:  Natural Language Processing with Deep Learning","text":"<ul> <li>Word Vectors - 1 </li> <li>Word Vectors - 2 </li> <li>Neural Networks</li> <li>Dependency Parsing</li> <li>Language Models </li> <li>Neural Machine Translation</li> <li>Question Answering</li> <li>Recursive Neural Networks</li> <li>Transformers</li> </ul>"},{"location":"NLP/Courses/stanford/#cs-324-large-language-models","title":"CS 324: Large Language Models","text":"<ul> <li>Introduction</li> <li>Data</li> <li>Modeling</li> <li>Training</li> <li>Parallelism</li> <li>Selective Architecture</li> <li>Adaptation</li> </ul>"},{"location":"NLP/LLM/LLAMA/3.pretraining/","title":"3. PreTraining","text":"<p>Section 3 of the paper provides an in-depth look at the pre-training process for LLaMA 3, covering data collection and curation, model architecture, scaling laws, and the infrastructure supporting the training process. Here\u2019s a detailed breakdown of each part:</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#31-pre-training-data","title":"3.1 Pre-Training Data","text":""},{"location":"NLP/LLM/LLAMA/3.pretraining/#data-collection-and-curation","title":"Data Collection and Curation","text":"<ul> <li>Data Sources<ul> <li>The pre-training dataset for LLaMA 3 is collected from a variety of sources, focusing on high-quality and diverse text. It includes web data, books, scientific papers, code repositories, and other multilingual resources.</li> </ul> </li> <li>Data Cleaning: Extensive measures are taken to clean the data. The process includes:<ul> <li>De-duplication: Removing duplicate content at various levels (URL, document, and line-level) to avoid overfitting and bias.</li> <li>Filtering: Heuristic and model-based filtering techniques are employed to remove low-quality documents, such as those containing adult content, excessive repetitions, or personal information (PII).</li> <li>Code and Math Data: Specialized pipelines are developed to extract code and math-related data, which are essential for tasks requiring reasoning and coding.</li> </ul> </li> </ul>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#determining-the-data-mix","title":"Determining the Data Mix","text":"<ul> <li>Knowledge Classification<ul> <li>To ensure a balanced training dataset, a classifier is used to categorize the collected web data into various knowledge domains. This allows for better control over the proportion of different types of data (e.g., general knowledge, reasoning, code).</li> </ul> </li> <li>Scaling Laws for Data Mix<ul> <li>Small-scale models are trained on different data mixes to predict the optimal data distribution for the full-scale model. This helps determine the best mix of data sources for training the final LLaMA 3 models.</li> </ul> </li> </ul>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#annealing-data","title":"Annealing Data","text":"<p>Towards the end of pre-training, the learning rate is annealed (gradually reduced) while focusing on high-quality data sources such as math and coding content. This stage improves the model\u2019s performance on specific tasks like mathematical reasoning, though gains diminish for the largest model.</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#model-architecture","title":"Model Architecture","text":"<p>LLaMA 3 adopts a standard dense Transformer architecture, similar to LLaMA 2, with incremental improvements to handle larger data and longer context windows:</p> <ul> <li>Grouped Query Attention (GQA): LLaMA 3 uses GQA, which improves both inference speed and memory efficiency. The attention mechanism is divided into multiple key-value pairs to optimize memory usage, particularly during decoding.</li> <li>Longer Context: The model supports a context window of up to 128K tokens, significantly larger than most language models. This is achieved by progressively increasing the context length during pre-training, ensuring the model can handle long documents efficiently.</li> <li>Vocabulary: A larger vocabulary size of 128,000 tokens is used, which includes 100K tokens from the OpenAI tiktoken tokenizer and 28K additional tokens for better multilingual support. This improves both compression rates and downstream task performance, without negatively impacting English text processing.</li> <li>Rotary Positional Embeddings (RoPE): The model uses RoPE for positional embeddings, which allows better scaling to long sequences. The base frequency hyperparameter for RoPE is set to 500,000, enabling efficient handling of very long contexts.</li> </ul>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#scaling-laws","title":"Scaling Laws","text":"<p>Scaling laws are mathematical models used to predict the optimal size of the model and the amount of data required to achieve certain levels of performance given a specific compute budget. For LLaMA 3:</p> <ul> <li>Compute-Optimal Model: The team conducts extensive experiments to develop scaling laws that guide the training of the compute-optimal model. These experiments show that LLaMA 3\u2019s 405 billion parameter model is an optimal balance between model size, training compute, and dataset size.</li> <li>FLOPs and Training Tokens: LLaMA 3 was trained using 3.8 \u00d7 10\u00b2\u2075 floating point operations (FLOPs) on 15.6 trillion tokens, an increase of nearly 50 times the training scale used for LLaMA 2. The scaling law experiments show that as the compute budget increases, performance becomes robust to small changes in model size and training tokens.</li> </ul> <p>The scaling law analysis also helps forecast performance on downstream tasks like ARC Challenge (a reasoning benchmark), providing valuable insights for model design.</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#infrastructure-scaling-and-efficiency","title":"Infrastructure, Scaling, and Efficiency","text":""},{"location":"NLP/LLM/LLAMA/3.pretraining/#training-infrastructure","title":"Training Infrastructure","text":"<ul> <li>Hardware: LLaMA 3 is trained on 16,000 H100 GPUs, each equipped with 80GB of high-bandwidth memory (HBM3). The training takes place on Meta\u2019s high-performance AI server platform, Grand Teton, which uses a combination of RDMA over Converged Ethernet (RoCE) and Nvidia Infiniband networks.</li> <li>Storage: A distributed file system, Tectonic, is used for data storage, providing 240 petabytes of capacity and supporting high-throughput operations of up to 7 terabytes per second.</li> <li>Network: The cluster is designed with 24K GPUs, connected through a three-layer Clos network that optimizes communication latency and throughput. Enhanced-ECMP (E-ECMP) routing ensures efficient load balancing across the network.</li> </ul>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#parallelism-for-model-scaling","title":"Parallelism for Model Scaling","text":"<p>4D Parallelism: LLaMA 3 employs 4D parallelism, which includes tensor, pipeline, context, and data parallelism to efficiently distribute training across thousands of GPUs:</p> <ul> <li>Tensor Parallelism (TP): Splits the model weights across mult iple devices. This allows the GPUs to collaboratively perform the computations for a single operation by working on different parts of the tensor simultaneously</li> <li>Pipeline Parallelism (PP): Divides the model vertically into stages for concurrent execution. The model's layers are sequentially partitioned, so different GPUs can process different layers of the model simultaneously</li> <li>Context Parallelism (CP): Splits long input sequences across devices, improving memory efficiency for long-context training. The input sequence is divided into chunks, and each chunk is processed in parallel by different GPUs. </li> <li>Data Parallelism (DP): Processes data in parallel across multiple GPUs while synchronizing after each step. Each GPU processes a different mini-batch of data and computes gradients independently. </li> </ul> <p>In LLaMA 3, these four types of parallelism are integrated in the following way:</p> <p>Tensor Parallelism (TP) is applied within each layer of the model to distribute the computation of large tensor operations across GPUs. Pipeline Parallelism (PP) is used to distribute different layers (or groups of layers) of the model across GPUs, allowing the model to be split across multiple devices. Context Parallelism (CP) is used to manage long sequences by splitting them across GPUs, allowing the model to handle inputs that are longer than what a single GPU can process. Data Parallelism (DP) is applied at the outermost level, distributing the training data across GPUs and synchronizing the updates to the model parameters</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#collective-communication","title":"Collective Communication","text":"<p>NCCLX: LLaMA 3 uses a custom fork of the Nvidia NCCL library, NCCLX, which is optimized for high-latency networks. This ensures efficient communication between GPUs across a massive cluster, handling issues like congestion and slowdowns through enhanced algorithms and priority mechanisms for critical data transfers.</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#training-recipe","title":"Training Recipe","text":"<p>The pre-training process for LLaMA 3 consists of three main stages:</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#initial-pre-training","title":"Initial Pre-Training","text":"<ul> <li>Training Recipe: LLaMA 3 is initially pre-trained using the AdamW optimizer with a peak learning rate of 8\u00d710\u207b\u2075 and a cosine learning rate schedule that decays over 1.2 million steps. The batch size is gradually increased from 4M to 16M tokens over the course of training to improve stability and efficiency.</li> <li>Adjusting Data Mix: Throughout training, the team adjusts the data mix to improve model performance, particularly in areas like multilingual understanding and mathematical reasoning.</li> </ul>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#long-context-pre-training","title":"Long-Context Pre-Training","text":"<p>Context Length Scaling: After the initial training stage, LLaMA 3 undergoes long-context pre-training to adapt to sequences of up to 128K tokens. This is done progressively, starting from 8K tokens and gradually increasing context lengths in stages, ensuring the model learns to handle large contexts without sacrificing performance on shorter contexts.</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#annealing","title":"Annealing","text":"<p>Annealing and Checkpoint Averaging: During the final stages of training, the learning rate is linearly annealed to zero while training on the last 40M tokens. Polyak averaging (averaging model checkpoints) is used during this phase to produce the final pre-trained model, ensuring stability and robustness.</p>"},{"location":"NLP/LLM/LLAMA/3.pretraining/#conclusion","title":"Conclusion","text":"<p>In summary, Section 3 of the paper provides a comprehensive overview of the LLaMA 3 pre-training process, covering everything from data collection and architecture design to scaling strategies and infrastructure optimizations. It emphasizes the importance of a well-balanced data mix, large-scale compute, and efficient parallelism to train a model capable of handling a wide range of tasks, including reasoning, coding, and multilingual processing.</p>"},{"location":"NLP/LLM/LLAMA/4.posttraining/","title":"4. Post Training","text":""},{"location":"NLP/LLM/LLAMA/4.posttraining/#introduction","title":"Introduction","text":"<p>Section 4 of the paper discusses post-training, which involves additional fine-tuning steps on a pre-trained LLaMA 3 model to improve its alignment with human instructions, preferences, and downstream task performance. Post-training is key to enhancing the model\u2019s instruction-following abilities, factual accuracy, tool use, and reasoning capabilities.</p>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#post-training-process","title":"Post-Training Process","text":"<p>The post-training pipeline includes:</p> <ul> <li>Supervised Fine-Tuning (SFT): The model is fine-tuned on examples either gathered through human annotation or generated synthetically. The SFT process involves teaching the model to better follow human instructions by learning from labeled data. This stage applies a cross-entropy loss to the model output.</li> <li>Direct Preference Optimization (DPO): After SFT, the model is further trained using preference data (human judgments) to optimize responses. DPO helps align model outputs with user preferences by learning directly from comparisons between different responses.</li> <li>Rejection Sampling: This technique is used during SFT to curate data, removing low-quality model generations and focusing training on the highest-quality examples.</li> </ul>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#chat-dialog-format","title":"Chat Dialog Format","text":"<p>To optimize the model for human-AI interactions in a conversational setting, the model is trained on chat dialogues with a specific protocol for exchanging human instructions and AI responses. LLaMA 3\u2019s capabilities, such as tool use, might require generating and sending multiple messages within a single conversational turn.</p>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#reward-modeling","title":"Reward Modeling","text":"<p>A reward model is trained to guide the model during SFT. This reward model scores responses based on human preferences, helping improve model performance on tasks such as factual accuracy and user alignment. Training the reward model includes handling pairs of responses labeled as \u201cchosen\u201d or \u201crejected\u201d by human annotators and sometimes involves a third option\u2014an edited version of the chosen response for further refinement.</p>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#supervised-fine-tuning-sft","title":"Supervised Fine-Tuning (SFT)","text":"<p>SFT takes human-labeled prompts and responses and fine-tunes the model using a standard cross-entropy loss. The data used in this process includes both human-generated and synthetic examples, targeting various skills such as code generation, reasoning, and tool use. For the largest LLaMA 3 models (405B parameters), SFT is performed over approximately 8,500 to 9,000 steps with optimized learning rates.</p>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#direct-preference-optimization-dpo","title":"Direct Preference Optimization (DPO)","text":"<p>DPO aligns the model by optimizing for human preferences in a more direct way than SFT. This method helps fine-tune model outputs based on what humans prefer, improving the quality and alignment of generated responses. DPO is applied in multiple rounds to ensure the model's outputs meet user expectations and instructions.</p>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#post-training-data","title":"Post-Training Data","text":"<p>The data used for post-training comes from a variety of sources:</p> <ul> <li>Human-Annotated Data: Human annotators generate and review prompts and responses, providing examples that help teach the model better instruction-following and reasoning abilities.</li> <li>Synthetic Data: LLaMA 3 and earlier models are used to generate synthetic data, which is a cost-effective way to augment the dataset. Synthetic data includes both single-turn examples and multi-turn dialogue examples for more complex tasks.</li> <li>Data Processing and Quality Control: Various filtering and quality-control mechanisms are applied to ensure the dataset used for post-training is diverse and high-quality, improving model performance across different areas like coding, factuality, and multilingual understanding.</li> </ul>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#capabilities","title":"Capabilities","text":"<p>Post-training helps to enhance several key capabilities of LLaMA 3, including:</p> <ul> <li>Reasoning and Coding: The model is specifically fine-tuned to perform well on reasoning benchmarks and code generation tasks, improving its problem-solving abilities and code-writing accuracy.</li> <li>Tool Use: The model learns to interact with external tools (e.g., function calls, APIs) through specialized training on function-calling tasks.</li> <li>Factuality: Post-training involves methods to reduce hallucinations and improve factual accuracy, aligning the model with real-world data and ensuring it knows when to defer answers to avoid incorrect information.</li> <li>Multilingual Performance: The model is fine-tuned to handle tasks in multiple languages, improving its capabilities across diverse linguistic contexts.</li> <li>Long Context: LLaMA 3 is trained to handle long-context tasks, leveraging its large 128K token context window for better understanding of longer documents.</li> </ul>"},{"location":"NLP/LLM/LLAMA/4.posttraining/#conclusion","title":"Conclusion","text":"<p>In conclusion, section 4 of the paper details a comprehensive post-training strategy aimed at refining the LLaMA 3 models through human feedback, specialized finetuning, and advanced optimization techniques like DPO, ultimately enhancing the model's ability to follow instructions, reason, generate accurate information, and handle complex tasks like tool use.</p>"},{"location":"NLP/LLM/LLAMA/6.Inference/","title":"6. Inference","text":""},{"location":"NLP/LLM/LLAMA/6.Inference/#introduction","title":"Introduction","text":"<p>This section discusses the optimization techniques and strategies used to make inference with the LLaMA 3 405B model more efficient. Since LLaMA 3 is a massive model with over 405 billion parameters, running inference efficiently requires substantial optimization to handle the computational load. Two main techniques are highlighted: pipeline parallelism and FP8 quantization.</p>"},{"location":"NLP/LLM/LLAMA/6.Inference/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Pipeline Parallelism is critical for managing the memory and computational requirements of large models like LLaMA 3. When using BF16 (16-bit floating-point) precision for the model parameters, the LLaMA 3 model (specifically the 405B variant) cannot fit into the GPU memory of a single machine equipped with 8 Nvidia H100 GPUs.</p> <ul> <li> <p>Challenges and Solutions:</p> <ul> <li> <p>Memory Constraints: A single machine cannot accommodate the model's parameters, which means the model needs to be spread across multiple GPUs and machines. To address this, BF16 precision is parallelized across 16 GPUs spread over two machines.</p> </li> <li> <p>Within-Machine Optimization: NVLink Bandwidth: High NVLink bandwidth within each machine allows the use of tensor parallelism, meaning the model parameters are distributed across multiple GPUs in the same machine. Cross-Machine Optimization:</p> </li> <li> <p>Pipeline Parallelism: For communication between machines, where connectivity has lower bandwidth and higher latency, pipeline parallelism is employed. This approach splits the model into multiple stages and allows forward passes to happen in parallel, increasing efficiency.</p> </li> </ul> </li> <li> <p>Bubbles Issue</p> <ul> <li>During training, pipeline bubbles\u2014idle periods while waiting for data\u2014are a major concern, as they reduce efficiency. However, in inference, these bubbles are not as problematic since there is no backward pass.</li> </ul> </li> <li> <p>Micro-Batching:</p> <ul> <li>Micro-batching improves throughput during inference by splitting the workload into smaller batches that can be processed in parallel.</li> <li>Evaluations were conducted with workloads of 4,096 input tokens and 256 output tokens during two stages:<ul> <li>Pre-fill Stage: Where the key-value cache is initialized.</li> <li>Decoding Stage: During token generation.</li> </ul> </li> <li>Micro-batching enables concurrent execution of micro-batches, improving overall throughput, although at the cost of slightly higher latency. Despite this trade-off, it offers a better throughput-latency trade-off.</li> </ul> </li> </ul>"},{"location":"NLP/LLM/LLAMA/6.Inference/#fp8-quantization","title":"FP8 Quantization","text":"<p>Quantization is another critical strategy used to make LLaMA 3 more efficient during inference. FP8 (8-bit floating-point) quantization leverages the native FP8 support on Nvidia H100 GPUs, enabling low-precision inference. This significantly reduces the computational and memory requirements.</p> <ul> <li> <p>Quantization Strategy:</p> <ul> <li>Matrix Multiplications:<ul> <li>FP8 quantization is applied to most matrix multiplications in the model, especially in the feedforward network layers. These layers account for approximately 50% of the inference compute time.</li> <li>The parameters in the self-attention layers are not quantized, as these layers are more sensitive to precision loss.</li> </ul> </li> <li> <p>Dynamic Scaling Factors:</p> <ul> <li>The quantization process uses dynamic scaling factors for better accuracy. This is essential to ensure that scaling factors are adjusted dynamically based on the data being processed. Custom CUDA kernels were developed to reduce the overhead of calculating these scaling factors.<ul> <li>Avoiding Quantization in Specific Layers:</li> </ul> </li> <li>The first and last Transformer layers are excluded from quantization, similar to the approach by Zhang et al. (2021). This is done because these layers are more prone to errors when quantized.</li> </ul> </li> </ul> </li> <li> <p>Handling High-Perplexity Tokens:</p> <ul> <li>Certain tokens, such as dates, can lead to large activation values, causing the dynamic scaling factors to become excessively high in FP8. To manage this, scaling factors are upper-bounded to 1200 to prevent underflows, which can lead to decoding errors.</li> </ul> </li> <li> <p>Row-wise Quantization:</p> <ul> <li>Instead of quantizing entire tensors, the model uses row-wise quantization, computing scaling factors across rows of the parameter and activation matrices. This provides finer control over quantization and helps to maintain high accuracy during inference. This approach is illustrated with results in Figure 25 in the paper, showing how row-wise quantization offers more granular control compared to tensor-wise quantization.</li> </ul> </li> <li> <p>Impact on Model Output Quality:</p> <ul> <li>Evaluations show that FP8 quantization has a negligible impact on the model\u2019s responses. A detailed analysis of reward model scores for 100,000 responses using both BF16 and FP8 shows almost identical distributions, indicating minimal degradation in model quality. The comparison between BF16 and FP8 inference is shown in Figure 26, which confirms that the FP8 quantization approach does not significantly affect the model\u2019s performance.</li> </ul> </li> <li> <p>Efficiency Gains:</p> <ul> <li>Experiments show that using FP8 inference provides a 50% improvement in throughput during the pre-fill stage and a substantially better throughput-latency trade-off during decoding (as shown in Figure 27). This makes FP8 quantization a highly effective technique for optimizing the inference process in LLaMA 3, especially for large models like the 405B variant.</li> </ul> </li> </ul>"},{"location":"NLP/LLM/LLAMA/6.Inference/#conclusion","title":"Conclusion","text":"<p>Section 6 of the paper presents two key optimization techniques for efficient inference in LLaMA 3: pipeline parallelism and FP8 quantization. These strategies address the memory and computational challenges posed by large models, allowing LLaMA 3 to operate effectively on GPUs like the Nvidia H100. By using pipeline parallelism and micro-batching, the model can scale across multiple GPUs while maintaining high throughput. FP8 quantization further reduces the resource requirements without compromising model quality, making LLaMA 3 both powerful and efficient for real-world applications.</p>"},{"location":"NLP/LLM/LLAMA/7.vision/","title":"7. Vision","text":""},{"location":"NLP/LLM/LLAMA/7.vision/#introduction","title":"Introduction","text":"<p>Section 7 of the paper introduces the vision experiments conducted for LLaMA 3, which incorporate visual-recognition capabilities into the model. The approach to integrating vision into the model is compositional, combining pre-existing image encoders and language models with cross-attention layers to fuse image and text data. This method enables the model to handle both image-text and video-text inputs effectively.</p>"},{"location":"NLP/LLM/LLAMA/7.vision/#data-collection-and-processing","title":"Data Collection and Processing","text":"<p>The vision experiments rely on two main types of data: images and videos.</p> <ul> <li>Image Data: The image encoder and adapter are trained using a dataset of 6 billion image-text pairs. The data processing pipeline includes:<ul> <li>Quality filtering: Removing low-quality image-text pairs by filtering captions using heuristics like CLIP scores.</li> <li>Perceptual de-duplication: To avoid training on redundant data, advanced de-duplication techniques are used, which cluster and compare embeddings.</li> <li>Resampling: Ensures diverse image-text pairs are used in training, improving performance in low-frequency categories.</li> <li>Optical Character Recognition (OCR): Helps improve the model's ability to understand and interpret text within images.</li> </ul> </li> <li>Video Data: For video pre-training, a dataset of video-text pairs is curated and cleaned through several stages:<ul> <li>Filtering and cleaning: Text is cleaned using rule-based heuristics, and videos with excessive overlaid text are removed.</li> <li>Contrastive filtering: CLIP-like models are used to align video-text pairs, filtering out low-similarity pairs.</li> <li>Motion-based filtering: Videos with low motion are excluded to ensure proper alignment between the text and the video content.</li> </ul> </li> </ul>"},{"location":"NLP/LLM/LLAMA/7.vision/#model-architecture","title":"Model Architecture","text":"<p>The architecture for the vision recognition model consists of three key components:</p> <ul> <li> <p>Image Encoder: A Vision Transformer (ViT-H/14) trained on 2.5 billion image-text pairs serves as the image encoder. This encoder processes images and aligns them with text representations.</p> </li> <li> <p>Image Adapter: Cross-attention layers are inserted between the image token representations (produced by the image encoder) and the language token representations from the pre-trained LLaMA 3 model. These cross-attention layers, with 100 billion parameters in the 405B model, facilitate the integration of visual information with textual information.</p> </li> <li> <p>Video Adapter: Temporal aggregator layers and video cross-attention layers are added for video-text pairs. These layers help the model understand and process temporal data from videos.</p> </li> </ul>"},{"location":"NLP/LLM/LLAMA/7.vision/#pre-training-and-scaling","title":"Pre-Training and Scaling","text":"<ul> <li> <p>The vision components are pre-trained in two stages:</p> <ul> <li>Initial Pre-Training: The image adapter is pre-trained on 6 billion image-text pairs with images resized to fit within tiles of 336x336 pixels.</li> <li>Annealing: Following initial training, the image adapter is fine-tuned on a higher-resolution dataset of 500 million images.</li> <li>Scaling: For video pre-training, similar strategies are used to scale the model. A key innovation is sampling a uniform number of video frames and adding cross-attention layers to capture temporal relationships.</li> </ul> </li> </ul>"},{"location":"NLP/LLM/LLAMA/7.vision/#post-training","title":"Post-Training","text":"<p>Post-training focuses on refining the vision capabilities through supervised fine-tuning (SFT) and optimization techniques like Direct Preference Optimization (DPO):</p> <ul> <li> <p>Supervised Fine-Tuning (SFT): Both image and video adapters are fine-tuned using highly curated multi-modal conversational data. This includes human annotations and synthetic data generated using a text-input LLM, which aids in generating diverse question-answer pairs related to images and videos.</p> </li> <li> <p>Direct Preference Optimization (DPO): This technique refines model outputs by training it on pairwise preference data, where annotators label responses as \"chosen\" or \"rejected.\"</p> </li> </ul>"},{"location":"NLP/LLM/LLAMA/7.vision/#image-recognition-results","title":"Image Recognition Results","text":"<p>LLaMA 3\u2019s vision model is evaluated on multiple image-recognition benchmarks, including tasks like:</p> <ul> <li>VQAv2: Focuses on answering questions about natural images.</li> <li>DocVQA: Tests document analysis and OCR understanding.</li> <li>ChartQA: Evaluates the model\u2019s ability to understand and answer questions about charts and visual data.</li> </ul> <p>Results indicate that the LLaMA 3 model with 405B parameters performs competitively across all benchmarks, often outperforming GPT-4V on various tasks, while being slightly behind in certain areas compared to competitors like Claude 3.5 Sonnet.</p>"},{"location":"NLP/LLM/LLAMA/7.vision/#video-recognition-results","title":"Video Recognition Results","text":"<p>LLaMA 3\u2019s video adapter is tested on temporal and causal reasoning benchmarks, such as:</p> <ul> <li>PerceptionTest: Evaluates the model's understanding of skills like memory and abstraction in video-based reasoning tasks.</li> <li>NExT-QA: Focuses on causal reasoning and answering questions based on video content.</li> <li>TVQA: Assesses the model\u2019s ability to answer questions based on both visual and subtitle data from TV shows.</li> </ul> <p>The results show that LLaMA 3 performs well in video understanding, sometimes outperforming other state-of-the-art models on certain benchmarks.</p>"},{"location":"NLP/LLM/LLAMA/7.vision/#conclusion","title":"Conclusion","text":"<p>In conclusion, Section 7 details how the vision capabilities of LLaMA 3 were developed and tested, showcasing its ability to handle complex image-text and video-text tasks efficiently. Through careful data curation, a compositional model architecture, and advanced training techniques, LLaMA 3 delivers competitive performance in multimodal benchmarks, positioning it as a strong competitor in the domain of vision-language models.</p>"},{"location":"NLP/LLM/LLAMA/8.speech/","title":"8. Speech","text":"<p>The Speech Experiments section in your document explores Llama 3's ability to understand and generate speech using compositional methods, similar to how visual recognition was integrated. Here's a detailed breakdown of the key areas:</p>"},{"location":"NLP/LLM/LLAMA/8.speech/#speech-understanding","title":"Speech Understanding","text":"<p>The paper explains how a speech encoder, combined with an adapter, processes speech input in Llama 3. The model can perform different speech tasks depending on the system prompt:</p> <ul> <li>Automatic Speech Recognition (ASR): Converts spoken language into text. Automatic Speech Translation (AST): Translates spoken language into another language.</li> <li>Spoken Dialogue: The model can act as a general spoken dialogue system, answering questions or having conversations with users.</li> </ul> <p>The speech interface supports 34 languages and can handle text and speech inputs together, making it adept at solving complex audio comprehension tasks.</p>"},{"location":"NLP/LLM/LLAMA/8.speech/#data-for-speech-understanding","title":"Data for Speech Understanding","text":"<ul> <li>The pre-training data consists of a vast amount of unlabeled speech (15 million hours), using self-supervised learning to initialize the speech encoder.</li> <li>Supervised finetuning involves specific speech recognition, translation, and spoken dialogue data (230K hours for ASR and 90K hours for AST), which enhances the model's performance in different tasks.</li> </ul>"},{"location":"NLP/LLM/LLAMA/8.speech/#model-architecture","title":"Model Architecture","text":"<ul> <li>The speech encoder is based on a Conformer model with 1 billion parameters. It processes 80-dimensional mel-spectrogram features, which are first downsampled to 40 ms intervals. The encoder consists of 24 layers of Conformer blocks, each with a latent dimension of 1536 and rotary attention modules using 24 attention heads.</li> <li>After encoding, a speech adapter processes the data. This adapter contains 100 million parameters and applies a convolution layer (kernel size of 3, stride of 2) to further reduce the frame length to 80 ms. A rotary Transformer layer and linear layer follow, mapping the speech embeddings to match the Llama 3 language model's embeddings</li> </ul>"},{"location":"NLP/LLM/LLAMA/8.speech/#speech-generation","title":"Speech Generation","text":"<p>Llama 3 also supports speech generation through a streaming text-to-speech (TTS) system that creates real-time speech waveforms as the model decodes text. The paper emphasizes the following:</p> <ul> <li>Text Normalization (TN): This ensures the model transforms written text into correct spoken form depending on context, such as reading numbers as digits or words.</li> <li>Prosody Modeling (PM): This improves the naturalness and expressiveness of generated speech by predicting key features like phone duration and pitch using Llama 3 embeddings.</li> </ul>"},{"location":"NLP/LLM/LLAMA/8.speech/#training-and-inference-for-speech-tasks","title":"Training and Inference for Speech Tasks","text":"<p>Training for Speech Understanding: The model undergoes a two-stage training process: Pre-training with unlabeled speech for generalization. Supervised fine-tuning for specific speech tasks while the language model remains frozen. Speech Generation: Uses a delayed pattern to capture long-range prosodic dependencies, improving latency and responsiveness for real-time synthesis.</p>"},{"location":"NLP/LLM/LLAMA/8.speech/#speech-understanding-and-generation-results","title":"Speech Understanding and Generation Results","text":"<p>The paper reports that Llama 3's performance in ASR, AST, and spoken dialogue is strong. On benchmarks such as Multilingual LibriSpeech (MLS) and FLEURS, the model outperforms other specialized systems like Whisper and SeamlessM4T. In terms of prosody modeling, Llama 3's ability to stream token-by-token allows for fast and natural speech synthesis. Results indicate a preference for Llama 3's prosody model over baseline models, with a significant improvement in the perceived quality of generated speech\u200b.</p>"}]}